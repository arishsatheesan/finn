{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5fc4fbc6-69d3-4dad-a800-83dba289d582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target device: cpu\n"
     ]
    }
   ],
   "source": [
    "# https://xilinx.github.io/brevitas/getting_started.html\n",
    "# works with automated folding\n",
    "\n",
    "from torch.nn import Module\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import brevitas.nn as qnn\n",
    "from brevitas.quant import Int8Bias as BiasQuant\n",
    "\n",
    "import os\n",
    "import onnx\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from brevitas.nn import QuantLinear, QuantReLU\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "weight_bit_width = 4\n",
    "act_bit_width = 4\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "class QuantWeightActLeNet(Module):\n",
    "    def __init__(self):\n",
    "        super(QuantWeightActLeNet, self).__init__()\n",
    "        self.quant_inp = qnn.QuantIdentity(bit_width=4)\n",
    "        self.conv1 = qnn.QuantConv2d(3, 6, 5, bias=True, weight_bit_width=weight_bit_width)\n",
    "        self.relu1 = qnn.QuantReLU(bit_width=act_bit_width)\n",
    "        self.conv2 = qnn.QuantConv2d(6, 16, 5, bias=True, weight_bit_width=weight_bit_width)\n",
    "        self.relu2 = qnn.QuantReLU(bit_width=act_bit_width)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1   = qnn.QuantLinear(16*10*10, 120, bias=True, weight_bit_width=weight_bit_width)\n",
    "        self.relu3 = qnn.QuantReLU(bit_width=act_bit_width)\n",
    "        self.fc2   = qnn.QuantLinear(120, 84, bias=True, weight_bit_width=weight_bit_width)\n",
    "        self.relu4 = qnn.QuantReLU(bit_width=act_bit_width)\n",
    "        self.fc3   = qnn.QuantLinear(84, 10, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.quant_inp(x)\n",
    "        out = self.relu1(self.conv1(out))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = self.relu2(self.conv2(out))\n",
    "        # out = F.max_pool2d(out, 2)\n",
    "        # out = self.quant_inp(out)\n",
    "        # out = out.reshape(out.shape[0], -1)\n",
    "        out = self.flatten(out)\n",
    "        out = self.relu3(self.fc1(out))\n",
    "        out = self.relu4(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     out = self.quant_inp(x)\n",
    "    #     out = self.relu1(self.conv1(out))\n",
    "    #     print(f\"Shape after conv1: {out.shape}\")\n",
    "    #     out = F.max_pool2d(out, 2)\n",
    "    #     out = self.relu2(self.conv2(out))\n",
    "    #     print(f\"Shape after conv2: {out.shape}\")\n",
    "    #     # out = F.max_pool2d(out, 2)\n",
    "    #     out = self.quant_inp(out)\n",
    "    #     # out = out.reshape(out.shape[0], -1)\n",
    "    #     out = self.flatten(out)\n",
    "    #     print(f\"Shape after flattening: {out.shape}\")\n",
    "    #     out = self.relu3(self.fc1(out))\n",
    "    #     print(f\"Shape after fc1: {out.shape}\")\n",
    "    #     out = self.relu4(self.fc2(out))\n",
    "    #     print(f\"Shape after fc2: {out.shape}\")\n",
    "    #     out = self.fc3(out)\n",
    "    #     return out\n",
    "\n",
    "model = QuantWeightActLeNet()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Target device: \" + str(device))\n",
    "\n",
    "model.to(device); # The semicolon is for not printing the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4edd2af4-0cfe-4fda-aec1-45a776ab5a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.nn import Module\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# import brevitas.nn as qnn\n",
    "# from brevitas.quant import Int8Bias #as BiasQuant\n",
    "\n",
    "# import os\n",
    "# import onnx\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# from torch.utils.data import TensorDataset\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# from brevitas.nn import QuantLinear, QuantReLU\n",
    "# import torch.nn as nn\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from tqdm import tqdm, trange\n",
    "\n",
    "# weight_bit_width = 8\n",
    "# act_bit_width = 8\n",
    "\n",
    "# torch.manual_seed(0)\n",
    "\n",
    "# class QuantWeightActLeNet(Module):\n",
    "#     def __init__(self):\n",
    "#         super(QuantWeightActLeNet, self).__init__()\n",
    "#         self.quant_inp = qnn.QuantIdentity(bit_width=8, return_quant_tensor=True)\n",
    "#         self.conv1 = qnn.QuantConv2d(in_channels=3, out_channels=10, kernel_size=(3,3), stride=(1,1), padding=(1,1), groups=1, dilation=1, bias=True, weight_bit_width=8, bias_quant=Int8Bias)\n",
    "#         self.relu1 = qnn.QuantReLU(bit_width=8, return_quant_tensor=True)\n",
    "#         self.conv2 = qnn.QuantConv2d(in_channels=10, out_channels=5, kernel_size=(3,3), stride=(1,1), padding=(1,1), groups=1, dilation=1, bias=True, weight_bit_width=8, bias_quant=Int8Bias)\n",
    "#         self.relu2 = qnn.QuantReLU(bit_width=8, return_quant_tensor=True)\n",
    "#         self.conv3 = qnn.QuantConv2d(in_channels=5, out_channels=1, kernel_size=(3,3), stride=(1,1), padding=(1,1), groups=1, dilation=1, bias=True, weight_bit_width=8, bias_quant=Int8Bias)\n",
    "#         self.relu3 = qnn.QuantReLU(bit_width=8, return_quant_tensor=True)\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.fc1 = qnn.QuantLinear(64, 128, bias=True, weight_bit_width=8, bias_quant=Int8Bias)\n",
    "#         self.relu4 = qnn.QuantReLU(bit_width=8, return_quant_tensor=True)\n",
    "#         self.fc2 = qnn.QuantLinear(128, 64, bias=True, weight_bit_width=8, bias_quant=Int8Bias)\n",
    "#         self.relu5 = qnn.QuantReLU(bit_width=8, return_quant_tensor=True)\n",
    "#         self.fc3 = qnn.QuantLinear(64, 10, bias=True, weight_bit_width=8, bias_quant=Int8Bias)\n",
    "    \n",
    "\n",
    "#     def forward(self, x): \n",
    "#         out = self.quant_inp(x)\n",
    "#         # print(f\"Input shape: {x.shape}\")\n",
    "#         out = self.conv1(out)\n",
    "#         # print(f\"Shape after conv1: {out.shape}\")\n",
    "#         out = self.relu1(out)\n",
    "#         out = F.max_pool2d(out, kernel_size=(2, 2), stride=(2, 2))\n",
    "#         out = self.conv2(out)\n",
    "#         # print(f\"Shape after conv2: {out.shape}\")\n",
    "#         out = self.relu2(out)\n",
    "#         out = F.max_pool2d(out, kernel_size=(2, 2), stride=(2, 2))\n",
    "#         out = self.conv3(out)\n",
    "#         # print(f\"Shape after conv3: {out.shape}\")\n",
    "#         out = self.relu3(out)\n",
    "#         out = self.flatten(out)\n",
    "#         # print(f\"Shape after flattening: {out.shape}\")\n",
    "#         # out = out.reshape(out.shape[0], -1)\n",
    "#         out = self.fc1(out)\n",
    "#         # print(f\"Shape after fc1: {out.shape}\")\n",
    "#         out = self.relu4(out)\n",
    "#         out = self.fc2(out)\n",
    "#         # print(f\"Shape after fc2: {out.shape}\")\n",
    "#         out = self.relu5(out)\n",
    "#         out = self.fc3(out)\n",
    "#         # print(f\"Shape after fc3: {out.shape}\")\n",
    "#         return out\n",
    "\n",
    "# model = QuantWeightActLeNet()\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(\"Target device: \" + str(device))\n",
    "\n",
    "# model.to(device); # The semicolon is for not printing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "948a70ad-6906-4811-a33f-37c6e0749edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.randn(1, 3, 32, 32)\n",
    "output = model(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ad4f53-4e45-4217-97f3-678171345246",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59b7b02a-8bb0-4dd3-b5e4-accf4aaa6f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Saved the dataset as .npz files\n"
     ]
    }
   ],
   "source": [
    "# This is for 4-bit quantization. The quantization value can be changed in line 11. \n",
    "\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "\n",
    "quant_param = 16 # log2(quant_param) bits. #256 means no quanization, 2 means 1-bit quantization\n",
    "\n",
    "def quantize_image(image):\n",
    "    \"\"\"Quantize and binarize an image.\"\"\"\n",
    "    image = image.astype(np.float32)\n",
    "    image = np.floor(image / (256/quant_param))  # Example: reducing to 4-bit quantization\n",
    "    return image.astype(np.float32)\n",
    "\n",
    "def save_dataset_as_npz(data, labels, filename):\n",
    "    \"\"\"Save the dataset as a .npz file.\"\"\"\n",
    "    np.savez_compressed(filename, data=data, labels=labels)\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Quantize the images\n",
    "train_images = np.array([quantize_image(image.numpy().transpose(1, 2, 0) * 255) for image, _ in train_dataset])\n",
    "train_labels = np.array(train_dataset.targets)\n",
    "\n",
    "test_images = np.array([quantize_image(image.numpy().transpose(1, 2, 0) * 255) for image, _ in test_dataset])\n",
    "test_labels = np.array(test_dataset.targets)\n",
    "\n",
    "# Save the datasets\n",
    "os.makedirs('./quantized_data', exist_ok=True)\n",
    "save_dataset_as_npz(train_images, train_labels, './quantized_data/cifar10_train.npz')\n",
    "save_dataset_as_npz(test_images, test_labels, './quantized_data/cifar10_test.npz')\n",
    "\n",
    "print('Saved the dataset as .npz files')\n",
    "\n",
    "\n",
    "class CIFAR10QuantizedDataset(Dataset):\n",
    "    def __init__(self, npz_file):\n",
    "        data = np.load(npz_file)\n",
    "        self.images = data['data']\n",
    "        self.labels = data['labels']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx].astype(np.float32) / 255.0\n",
    "        label = self.labels[idx]\n",
    "        image = torch.tensor(image.transpose(2, 0, 1))  # HWC to CHW format\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return image, label\n",
    "\n",
    "# Load the quantized dataset\n",
    "train_dataset = CIFAR10QuantizedDataset('./quantized_data/cifar10_train.npz')\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "test_dataset = CIFAR10QuantizedDataset('./quantized_data/cifar10_test.npz')\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d13af9-200e-4f88-9f38-5f35d18bb9ce",
   "metadata": {},
   "source": [
    "### Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5ba85b47-0263-4bdf-a0b0-7e76ce4dc7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss = 1.449655 test accuracy = 0.479300: 100%|█████████████████| 10/10 [01:30<00:00,  9.09s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train(model, train_loader, optimizer, criterion):\n",
    "    losses = []\n",
    "    # ensure model is in training mode\n",
    "    model.train()    \n",
    "    \n",
    "    for i, data in enumerate(train_loader, 0):        \n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()   \n",
    "                \n",
    "        # forward pass\n",
    "        output = model(images.float())\n",
    "        # loss = criterion(output, labels.unsqueeze(1))\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        # backward pass + run optimizer to update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # keep track of loss value\n",
    "        losses.append(loss.data.cpu().numpy()) \n",
    "    return losses\n",
    "\n",
    "\n",
    "def test(model, test_loader):    \n",
    "    # ensure model is in eval mode\n",
    "    model.eval() \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "   \n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            output = model(images.float())\n",
    "            # run the output through sigmoid\n",
    "            # output = torch.sigmoid(output_orig)  \n",
    "            # compare against a threshold of 0.5 to generate 0/1\n",
    "            # pred = (output.detach().cpu().numpy() > 0.5) * 1\n",
    "            _, pred = torch.max(output.data, 1)\n",
    "            labels = labels.cpu().float()\n",
    "            y_true.extend(labels.tolist()) \n",
    "            y_pred.extend(pred.reshape(-1).tolist())\n",
    "            # y_pred.extend((pred == labels).sum().item())\n",
    "        \n",
    "    return accuracy_score(y_true, y_pred)\n",
    "\n",
    "num_epochs = 10\n",
    "lr = 0.001 \n",
    "\n",
    "def display_loss_plot(losses, title=\"Training loss\", xlabel=\"Iterations\", ylabel=\"Loss\"):\n",
    "    x_axis = [i for i in range(len(losses))]\n",
    "    plt.plot(x_axis,losses)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()\n",
    "\n",
    "# loss criterion and optimizer\n",
    "# criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Setting seeds for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "running_loss = []\n",
    "running_test_acc = []\n",
    "t = trange(num_epochs, desc=\"Training loss\", leave=True)\n",
    "\n",
    "for epoch in t:\n",
    "        loss_epoch = train(model, train_loader, optimizer,criterion)\n",
    "        test_acc = test(model, test_loader)\n",
    "        t.set_description(\"Training loss = %f test accuracy = %f\" % (np.mean(loss_epoch), test_acc))\n",
    "        t.refresh() # to show immediately the update           \n",
    "        running_loss.append(loss_epoch)\n",
    "        running_test_acc.append(test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d0ba76-1444-4d26-a305-272c102f909c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b3e05908-29dc-4f6b-b3d4-67817d90be98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy =  0.4793\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = test(model, test_loader)\n",
    "print('test accuracy = ', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc3c92a8-51cc-48ab-b7dc-f253413e18ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the Brevitas model to disk\n",
    "# torch.save(model.state_dict(), \"state_dict_LeNet_WeightAct.pth\")\n",
    "# # torch.save(model, \"LeNet_WeightAct.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75516101-17f3-4577-ae05-f2c15844cf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class QuantWeightActLeNet(Module):\n",
    "#     def __init__(self):\n",
    "#         super(QuantWeightActLeNet, self).__init__()\n",
    "#         self.quant_inp = qnn.QuantIdentity(bit_width=4)\n",
    "#         self.conv1 = qnn.QuantConv2d(3, 6, 5, bias=True, weight_bit_width=weight_bit_width)\n",
    "#         self.relu1 = qnn.QuantReLU(bit_width=act_bit_width)\n",
    "#         self.conv2 = qnn.QuantConv2d(6, 16, 5, bias=True, weight_bit_width=weight_bit_width)\n",
    "#         self.relu2 = qnn.QuantReLU(bit_width=act_bit_width)\n",
    "#         self.fc1   = qnn.QuantLinear(16*5*5, 120, bias=True, weight_bit_width=weight_bit_width)\n",
    "#         self.relu3 = qnn.QuantReLU(bit_width=act_bit_width)\n",
    "#         self.fc2   = qnn.QuantLinear(120, 84, bias=True, weight_bit_width=weight_bit_width)\n",
    "#         self.relu4 = qnn.QuantReLU(bit_width=act_bit_width)\n",
    "#         self.fc3   = qnn.QuantLinear(84, 10, bias=True)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out = self.quant_inp(x)\n",
    "#         out = self.relu1(self.conv1(out))\n",
    "#         out = F.max_pool2d(out, 2)\n",
    "#         out = self.relu2(self.conv2(out))\n",
    "#         out = F.max_pool2d(out, 2)\n",
    "#         out = out.reshape(out.shape[0], -1)\n",
    "#         out = self.relu3(self.fc1(out))\n",
    "#         out = self.relu4(self.fc2(out))\n",
    "#         out = self.fc3(out)\n",
    "#         return out\n",
    "\n",
    "# model = QuantWeightActLeNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72396db9-ef9b-4f17-bfda-3e5c6e26184c",
   "metadata": {},
   "source": [
    "####  Convert to ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "864c180c-c4d7-496f-85f1-b11d9e74dbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inets/arish/FINN_arish/finn/deps/qonnx/src/qonnx/transformation/gemm_to_matmul.py:57: UserWarning: The GemmToMatMul transformation only offers explicit support for version 9 of the Gemm node, but the ONNX version of the supplied model is 14. Thus the transformation may fail or return incomplete results.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import onnx\n",
    "from finn.util.test import get_test_model_trained\n",
    "from brevitas.export import export_qonnx\n",
    "from qonnx.util.cleanup import cleanup as qonnx_cleanup\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from finn.transformation.qonnx.convert_qonnx_to_finn import ConvertQONNXtoFINN\n",
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "from qonnx.transformation.fold_constants import FoldConstants\n",
    "from qonnx.transformation.general import GiveReadableTensorNames, GiveUniqueNodeNames, RemoveStaticGraphInputs\n",
    "\n",
    "# # cnv = get_test_model_trained(\"CNV\", 1, 1)\n",
    "# model_path = \"state_dict_LeNet_WeightAct.pth\"\n",
    "# state_dict = torch.load(model_path)  # This returns an OrderedDict\n",
    "# model.load_state_dict(state_dict)  # Load the state dict into the model\n",
    "# # model = torch.load(model_path)\n",
    "\n",
    "export_onnx_path = \"LeNet_WeightAct.onnx\"\n",
    "export_qonnx(model, torch.randn(1, 3, 32, 32), export_onnx_path)\n",
    "qonnx_cleanup(export_onnx_path, out_file=export_onnx_path)\n",
    "\n",
    "model = ModelWrapper(export_onnx_path)\n",
    "model = model.transform(ConvertQONNXtoFINN())\n",
    "model = model.transform(InferShapes())\n",
    "model = model.transform(FoldConstants())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(GiveReadableTensorNames())\n",
    "model = model.transform(RemoveStaticGraphInputs())\n",
    "model.save(\"LeNet_WeightAct_tidy.onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8f355842-2a9f-4485-adda-11cf9d07ad8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving 'LeNet_WeightAct_tidy.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7afa647f0a00>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finn.util.visualization import showInNetron\n",
    "showInNetron(\"LeNet_WeightAct_tidy.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ee3894-2b2c-445d-bea0-670570592faa",
   "metadata": {},
   "source": [
    "### Adding pre and post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "44986f1e-c8c6-4481-9999-63c09cc14186",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inets/arish/FINN_arish/finn/deps/qonnx/src/qonnx/transformation/infer_data_layouts.py:127: UserWarning: Assuming 4D input is NCHW\n",
      "  warnings.warn(\"Assuming 4D input is NCHW\")\n"
     ]
    }
   ],
   "source": [
    "from finn.util.pytorch import ToTensor\n",
    "from qonnx.transformation.merge_onnx_models import MergeONNXModels\n",
    "from qonnx.core.datatype import DataType\n",
    "\n",
    "model = ModelWrapper(\"LeNet_WeightAct_tidy.onnx\")\n",
    "global_inp_name = model.graph.input[0].name\n",
    "ishape = model.get_tensor_shape(global_inp_name)\n",
    "# preprocessing: torchvision's ToTensor divides uint8 inputs by 255\n",
    "totensor_pyt = ToTensor()\n",
    "chkpt_preproc_name = \"LeNet_WeightAct_tidy_preproc.onnx\"\n",
    "export_qonnx(totensor_pyt, torch.randn(ishape), chkpt_preproc_name)\n",
    "qonnx_cleanup(chkpt_preproc_name, out_file=chkpt_preproc_name)\n",
    "pre_model = ModelWrapper(chkpt_preproc_name)\n",
    "pre_model = pre_model.transform(ConvertQONNXtoFINN())\n",
    "\n",
    "# join preprocessing and core model\n",
    "model = model.transform(MergeONNXModels(pre_model))\n",
    "# add input quantization annotation: UINT8 for all BNN-PYNQ models\n",
    "global_inp_name = model.graph.input[0].name\n",
    "model.set_tensor_datatype(global_inp_name, DataType[\"UINT8\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "66275de1-dc37-452c-9aef-b81137f07bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qonnx.transformation.insert_topk import InsertTopK\n",
    "from qonnx.transformation.infer_datatypes import InferDataTypes\n",
    "\n",
    "# postprocessing: insert Top-1 node at the end\n",
    "model = model.transform(InsertTopK(k=1))\n",
    "chkpt_name = \"LeNet_WeightAct_tidy_prepost.onnx\"\n",
    "# tidy-up again\n",
    "model = model.transform(InferShapes())\n",
    "model = model.transform(FoldConstants())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(GiveReadableTensorNames())\n",
    "model = model.transform(InferDataTypes())\n",
    "model = model.transform(RemoveStaticGraphInputs())\n",
    "model.save(chkpt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "722e1160-1447-4598-a77b-35a342d80cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving 'LeNet_WeightAct_tidy_prepost.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7afbcc66a020>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finn.util.visualization import showInNetron\n",
    "showInNetron(\"LeNet_WeightAct_tidy_prepost.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3943bc-ef1a-4c0f-ae15-40e99bd81549",
   "metadata": {},
   "source": [
    "### Streamline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9678c2b8-0246-4bfe-a7d1-55219124ed62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.streamline import Streamline\n",
    "from qonnx.transformation.lower_convs_to_matmul import LowerConvsToMatMul\n",
    "from qonnx.transformation.bipolar_to_xnor import ConvertBipolarMatMulToXnorPopcount\n",
    "import finn.transformation.streamline.absorb as absorb\n",
    "from finn.transformation.streamline.reorder import MakeMaxPoolNHWC, MoveScalarLinearPastInvariants\n",
    "from qonnx.transformation.infer_data_layouts import InferDataLayouts\n",
    "from qonnx.transformation.general import RemoveUnusedTensors\n",
    "import finn.transformation.fpgadataflow.convert_to_hw_layers as to_hw\n",
    "from qonnx.transformation.general import GiveReadableTensorNames, GiveUniqueNodeNames, RemoveStaticGraphInputs\n",
    "from finn.transformation.streamline.round_thresholds import RoundAndClipThresholds\n",
    "\n",
    "model = ModelWrapper(\"LeNet_WeightAct_tidy_prepost.onnx\")\n",
    "model = model.transform(MoveScalarLinearPastInvariants())\n",
    "model = model.transform(Streamline())\n",
    "model = model.transform(LowerConvsToMatMul())\n",
    "model = model.transform(MakeMaxPoolNHWC())\n",
    "model = model.transform(absorb.AbsorbTransposeIntoMultiThreshold())\n",
    "model = model.transform(ConvertBipolarMatMulToXnorPopcount())\n",
    "model = model.transform(Streamline())\n",
    "\n",
    "# model = model.transform(MakeMaxPoolNHWC())#\n",
    "# model = model.transform(absorb.AbsorbTransposeIntoMultiThreshold())#\n",
    "# # model = model.transform(ConvertBipolarMatMulToXnorPopcount())#\n",
    "# model = model.transform(Streamline())\n",
    "\n",
    "# model = model.transform(absorb.AbsorbAddIntoMultiThreshold())\n",
    "# model = model.transform(absorb.AbsorbMulIntoMultiThreshold())\n",
    "\n",
    "# absorb final add-mul nodes into TopK\n",
    "model = model.transform(absorb.AbsorbScalarMulAddIntoTopK())\n",
    "\n",
    "# model = model.transform(RoundAndClipThresholds())\n",
    "# model = model.transform(Streamline())\n",
    "# model = model.transform(GiveUniqueNodeNames())\n",
    "# model = model.transform(GiveReadableTensorNames())\n",
    "# model = model.transform(InferDataTypes())\n",
    "\n",
    "# bit of tidy-up\n",
    "model = model.transform(InferDataLayouts())\n",
    "model = model.transform(RemoveUnusedTensors())\n",
    "model.save(\"LeNet_WeightAct_tidy_streamlined.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2d3ef1e8-26a0-4549-9190-30410056422b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving 'LeNet_WeightAct_tidy_streamlined.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7afa62fc6470>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finn.util.visualization import showInNetron\n",
    "showInNetron(\"LeNet_WeightAct_tidy_streamlined.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a15c285-9466-4a88-bead-bb78189d9ec8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e6beb00-dd6b-47e5-a31c-7fef46d38f2f",
   "metadata": {},
   "source": [
    "### Partitioning, coversion of HW layers and folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81525a7f-f2fe-4df0-a5a5-ef69f04d6448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresh_node = model.get_nodes_by_op_type(\"Thresholding\")[0]\n",
    "# thresh_node_inst = getCustomOp(thresh_node)\n",
    "# thresh_node_inst.set_nodeattr(\"preferred_impl_style\", \"hls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3d3e5246-b3af-4a8b-a7e2-ed4a7571221c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Ultra96', 'Ultra96-V2', 'Pynq-Z1', 'Pynq-Z2', 'ZCU102', 'ZCU104', 'ZCU111', 'RFSoC2x2', 'RFSoC4x2', 'KV260_SOM'])\n"
     ]
    }
   ],
   "source": [
    "# # print the names of the supported PYNQ boards\n",
    "# from finn.util.basic import pynq_part_map\n",
    "# print(pynq_part_map.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ddac8c8d-c78d-4709-ba40-3bd7b36a40b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.util.basic import pynq_part_map\n",
    "# change this if you have a different PYNQ board, see list above\n",
    "pynq_board = \"Pynq-Z1\"\n",
    "fpga_part = pynq_part_map[pynq_board]\n",
    "target_clk_ns = 10\n",
    "\n",
    "import finn.transformation.fpgadataflow.convert_to_hw_layers as to_hw\n",
    "from finn.transformation.fpgadataflow.create_dataflow_partition import (\n",
    "    CreateDataflowPartition,\n",
    ")\n",
    "from finn.transformation.move_reshape import RemoveCNVtoFCFlatten\n",
    "from finn.transformation.fpgadataflow.specialize_layers import SpecializeLayers\n",
    "from qonnx.custom_op.registry import getCustomOp\n",
    "from qonnx.transformation.infer_data_layouts import InferDataLayouts\n",
    "\n",
    "model = ModelWrapper(\"LeNet_WeightAct_tidy_streamlined.onnx\")\n",
    "model = model.transform(to_hw.InferBinaryMatrixVectorActivation())\n",
    "model = model.transform(to_hw.InferQuantizedMatrixVectorActivation())\n",
    "# TopK to LabelSelect\n",
    "model = model.transform(to_hw.InferLabelSelectLayer())\n",
    "# input quantization (if any) to standalone thresholding\n",
    "model = model.transform(to_hw.InferThresholdingLayer())\n",
    "model = model.transform(to_hw.InferConvInpGen())\n",
    "model = model.transform(to_hw.InferStreamingMaxPool())\n",
    "# get rid of Reshape(-1, 1) operation between hw nodes\n",
    "model = model.transform(RemoveCNVtoFCFlatten())\n",
    "# get rid of Tranpose -> Tranpose identity seq\n",
    "model = model.transform(absorb.AbsorbConsecutiveTransposes())\n",
    "# infer tensor data layouts\n",
    "model = model.transform(InferDataLayouts())\n",
    "\n",
    "parent_model = model.transform(CreateDataflowPartition())\n",
    "parent_model.save(\"LeNet_WeightAct_tidy_dataflow_parent.onnx\")\n",
    "sdp_node = parent_model.get_nodes_by_op_type(\"StreamingDataflowPartition\")[0]\n",
    "sdp_node = getCustomOp(sdp_node)\n",
    "dataflow_model_filename = sdp_node.get_nodeattr(\"model\")\n",
    "# save the dataflow partition with a different name for easier access\n",
    "# and specialize the layers to HLS variants\n",
    "dataflow_model = ModelWrapper(dataflow_model_filename)\n",
    "dataflow_model = dataflow_model.transform(SpecializeLayers(fpga_part))\n",
    "dataflow_model.save(\"LeNet_WeightAct_dataflow_model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e64dcc5d-1a5c-4a71-a45f-133716901e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving 'LeNet_WeightAct_tidy_dataflow_parent.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7afbcd396770>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(\"LeNet_WeightAct_tidy_dataflow_parent.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9bd2bba5-cf39-446c-a352-2e7ee7f91736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving 'LeNet_WeightAct_dataflow_model.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7afbcc7d9db0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(\"LeNet_WeightAct_dataflow_model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e337ef-4b96-4b12-b3d9-5103b9cd74a5",
   "metadata": {},
   "source": [
    "### Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bc774ada-bbc8-479c-81e6-b1953a1c59bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> PE: 6, SIMD: 3, inFIFOdepths: [16]\n",
      " -> PE: 8, SIMD: 3, inFIFOdepths: [16]\n",
      " -> PE: 60, SIMD: 64, inFIFOdepths: [64]\n",
      " -> PE: 42, SIMD: 30, inFIFOdepths: [128]\n"
     ]
    }
   ],
   "source": [
    "from finn.transformation.fpgadataflow.make_zynq_proj import ZynqBuild\n",
    "model = ModelWrapper(\"LeNet_WeightAct_dataflow_model.onnx\")\n",
    "fc_layers = model.get_nodes_by_op_type(\"MVAU_hls\")\n",
    "# print(fc_layers)\n",
    "# each tuple is (PE, SIMD, in_fifo_depth) for a layer\n",
    "folding = [\n",
    "    (6, 3, [16]),   # conv1\n",
    "    (8, 3, [16]),   # conv2\n",
    "    (60, 64, [64]), # fc1\n",
    "    (42, 30, [128]), # fc2\n",
    "    (5, 42, [10]),   #fc3\n",
    "]\n",
    "\n",
    "# Apply folding parameters to fully connected layers\n",
    "for fcl, (pe, simd, ififodepth) in zip(fc_layers, folding):\n",
    "    fcl_inst = getCustomOp(fcl)\n",
    "    fcl_inst.set_nodeattr(\"PE\", pe)\n",
    "    fcl_inst.set_nodeattr(\"SIMD\", simd)\n",
    "    fcl_inst.set_nodeattr(\"inFIFODepths\", ififodepth)\n",
    "    # Debug: Verify parameters are set correctly\n",
    "    print(f\"{fcl.name} -> PE: {pe}, SIMD: {simd}, inFIFOdepths: {ififodepth}\")\n",
    "\n",
    "# use same SIMD values for the sliding window operators\n",
    "swg_layers = model.get_nodes_by_op_type(\"ConvolutionInputGenerator_rtl\")\n",
    "for i in range(len(swg_layers)):\n",
    "    swg_inst = getCustomOp(swg_layers[i])\n",
    "    simd = folding[i][1]\n",
    "    swg_inst.set_nodeattr(\"SIMD\", simd)\n",
    "\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model.save(\"LeNet_WeightAct_folded.onnx\")\n",
    "\n",
    "\n",
    "# for i, swg in enumerate(swg_layers):\n",
    "#   if i < len(folding):\n",
    "#       swg_inst = getCustomOp(swg)\n",
    "#       simd = folding[i][1]\n",
    "#       swg_inst.set_nodeattr(\"SIMD\", simd)\n",
    "#       # Debug: Verify SIMD is set correctly\n",
    "#       print(f\"{swg.name} -> SIMD: {simd}\")\n",
    "#   else:\n",
    "#       print(f\"No folding parameters available for {swg.name}\")\n",
    "      \n",
    "# # Save the transformed model\n",
    "# model = model.transform(GiveUniqueNodeNames())\n",
    "# model.save(\"LeNet_WeightAct_folded.onnx\")\n",
    "\n",
    "# # # set parallelism for input quantizer to be same as first layer's SIMD\n",
    "# # inp_qnt_node = model.get_nodes_by_op_type(\"Thresholding_hls\")[0]\n",
    "# # inp_qnt = getCustomOp(inp_qnt_node)\n",
    "# # inp_qnt.set_nodeattr(\"PE\", 49)\n",
    "  \n",
    "# # Debug: Print the shapes to ensure compatibility\n",
    "# for node in model.graph.node:\n",
    "#   node_inst = getCustomOp(node)\n",
    "#   try:\n",
    "#       print(f\"Checking folded input shape for node: {node.name}\")\n",
    "#       folded_shape = node_inst.get_folded_input_shape()\n",
    "#       print(f\"{node.name}: Folded input shape = {folded_shape}\")\n",
    "#   except Exception as e:\n",
    "#       print(f\"Error getting folded input shape for {node.name}: {e}\")\n",
    "\n",
    "# # Hardware generation parameters\n",
    "# test_pynq_board = \"Pynq-Z1\"\n",
    "# target_clk_ns = 10\n",
    "\n",
    "# # Transform and verify if parameters are applied\n",
    "# try:\n",
    "#   model = model.transform(ZynqBuild(platform=test_pynq_board, period_ns=target_clk_ns))\n",
    "#   print(\"Hardware generation successful\")\n",
    "# except Exception as e:\n",
    "#   print(f\"Error during hardware generation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0cb4905a-6aa1-4a16-897c-1c842d7b20f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving 'LeNet_WeightAct_folded.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7afbcc4c65c0>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(\"LeNet_WeightAct_folded.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3c76b3e6-fc36-40b6-876e-a70d2564086c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inets/arish/FINN_arish/finn/src/finn/transformation/fpgadataflow/floorplan.py:107: UserWarning: 17 nodes have no entry in the provided floorplan, SLR was set to -1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 1600 into shape (1,10,10,0,64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m target_clk_ns \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m ModelWrapper(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLeNet_WeightAct_folded.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mZynqBuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplatform\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpynq_board\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperiod_ns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtarget_clk_ns\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/inets/arish/FINN_arish/finn/deps/qonnx/src/qonnx/core/modelwrapper.py:140\u001b[0m, in \u001b[0;36mModelWrapper.transform\u001b[0;34m(self, transformation, make_deepcopy, cleanup)\u001b[0m\n\u001b[1;32m    138\u001b[0m model_was_changed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m model_was_changed:\n\u001b[0;32m--> 140\u001b[0m     (transformed_model, model_was_changed) \u001b[38;5;241m=\u001b[39m \u001b[43mtransformation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformed_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cleanup:\n\u001b[1;32m    142\u001b[0m     transformed_model\u001b[38;5;241m.\u001b[39mcleanup()\n",
      "File \u001b[0;32m/home/inets/arish/FINN_arish/finn/src/finn/transformation/fpgadataflow/make_zynq_proj.py:340\u001b[0m, in \u001b[0;36mZynqBuild.apply\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    338\u001b[0m dataflow_model_filename \u001b[38;5;241m=\u001b[39m sdp_node\u001b[38;5;241m.\u001b[39mget_nodeattr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    339\u001b[0m kernel_model \u001b[38;5;241m=\u001b[39m ModelWrapper(dataflow_model_filename)\n\u001b[0;32m--> 340\u001b[0m kernel_model \u001b[38;5;241m=\u001b[39m \u001b[43mkernel_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mInsertFIFO\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m kernel_model \u001b[38;5;241m=\u001b[39m kernel_model\u001b[38;5;241m.\u001b[39mtransform(SpecializeLayers(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfpga_part))\n\u001b[1;32m    342\u001b[0m kernel_model \u001b[38;5;241m=\u001b[39m kernel_model\u001b[38;5;241m.\u001b[39mtransform(GiveUniqueNodeNames(prefix))\n",
      "File \u001b[0;32m/home/inets/arish/FINN_arish/finn/deps/qonnx/src/qonnx/core/modelwrapper.py:140\u001b[0m, in \u001b[0;36mModelWrapper.transform\u001b[0;34m(self, transformation, make_deepcopy, cleanup)\u001b[0m\n\u001b[1;32m    138\u001b[0m model_was_changed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m model_was_changed:\n\u001b[0;32m--> 140\u001b[0m     (transformed_model, model_was_changed) \u001b[38;5;241m=\u001b[39m \u001b[43mtransformation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformed_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cleanup:\n\u001b[1;32m    142\u001b[0m     transformed_model\u001b[38;5;241m.\u001b[39mcleanup()\n",
      "File \u001b[0;32m/home/inets/arish/FINN_arish/finn/src/finn/transformation/fpgadataflow/insert_fifo.py:115\u001b[0m, in \u001b[0;36mInsertFIFO.apply\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    113\u001b[0m n0 \u001b[38;5;241m=\u001b[39m getCustomOp(first_node)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# determine fifo node attributes\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m fld_shape \u001b[38;5;241m=\u001b[39m \u001b[43mn0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_folded_output_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m dtype \u001b[38;5;241m=\u001b[39m n0\u001b[38;5;241m.\u001b[39mget_output_datatype()\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# check if folded_shape of output of first node and\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# input of the second node is equal\u001b[39;00m\n",
      "File \u001b[0;32m/home/inets/arish/FINN_arish/finn/src/finn/custom_op/fpgadataflow/streamingdatawidthconverter.py:124\u001b[0m, in \u001b[0;36mStreamingDataWidthConverter.get_folded_output_shape\u001b[0;34m(self, ind)\u001b[0m\n\u001b[1;32m    122\u001b[0m new_shape\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mint\u001b[39m(ochannels \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m oelems))\n\u001b[1;32m    123\u001b[0m new_shape\u001b[38;5;241m.\u001b[39mappend(oelems)\n\u001b[0;32m--> 124\u001b[0m dummy_t \u001b[38;5;241m=\u001b[39m \u001b[43mdummy_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dummy_t\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 1600 into shape (1,10,10,0,64)"
     ]
    }
   ],
   "source": [
    "from finn.transformation.fpgadataflow.make_zynq_proj import ZynqBuild\n",
    "test_pynq_board = \"Pynq-Z1\"\n",
    "target_clk_ns = 10\n",
    "model = ModelWrapper(\"LeNet_WeightAct_folded.onnx\")\n",
    "model = model.transform(ZynqBuild(platform = pynq_board, period_ns = target_clk_ns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c14ce0-c03e-4d53-8d2b-569c3a33d90b",
   "metadata": {},
   "source": [
    "## Understanding the Build Configuration: `DataflowBuildConfig` <a id=\"underst_build_conf\"></a>\n",
    "\n",
    "The build configuration is specified by an instance of `finn.builder.build_dataflow_config.DataflowBuildConfig`. The configuration is a Python [`dataclass`](https://docs.python.org/3/library/dataclasses.html) which can be serialized into or de-serialized from JSON files for persistence, although we'll just set it up in Python here.\n",
    "There are many options in the configuration to customize different aspects of the build, we'll only cover a few of them in this notebook. You can read the details on all the config options on [the FINN API documentation](https://finn-dev.readthedocs.io/en/latest/source_code/finn.builder.html#finn.builder.build_dataflow_config.DataflowBuildConfig).\n",
    "\n",
    "Let's go over some of the members of the `DataflowBuildConfig`:\n",
    "\n",
    "### Output Products <a id=\"output_prod\"></a>\n",
    "\n",
    "The build can produce many different outputs, and some of them can take a long time (e.g. bitfile synthesis for a large network). When you first start working on generating a new accelerator and exploring the different performance options, you may not want to go all the way to a bitfile. Thus, in the beginning you may just select the estimate reports as the output products. Gradually, you can generate the output products from later stages until you are happy enough with the design to build the full accelerator integrated into a shell.\n",
    "\n",
    "The output products are controlled by:\n",
    "\n",
    "* `generate_outputs`: list of output products (of type [`finn.builder.build_dataflow_config.DataflowOutputType`](https://finn-dev.readthedocs.io/en/latest/source_code/finn.builder.html#finn.builder.build_dataflow_config.DataflowOutputType)) that will be generated by the build. Some available options are:\n",
    "    - `ESTIMATE_REPORTS` : report expected resources and performance per layer and for the whole network without any synthesis\n",
    "    - `STITCHED_IP` : create a stream-in stream-out IP design that can be integrated into other Vivado IPI or RTL designs\n",
    "    - `RTLSIM_PERFORMANCE` : use PyVerilator to do a performance/latency test of the `STITCHED_IP` design\n",
    "    - `OOC_SYNTH` : run out-of-context synthesis (just the accelerator itself, without any system surrounding it) on the `STITCHED_IP` design to get post-synthesis FPGA resources and achievable clock frequency\n",
    "    - `BITFILE` : integrate the accelerator into a shell to produce a standalone bitfile\n",
    "    - `PYNQ_DRIVER` : generate a PYNQ Python driver that can be used to launch the accelerator\n",
    "    - `DEPLOYMENT_PACKAGE` : create a folder with the `BITFILE` and `PYNQ_DRIVER` outputs, ready to be copied to the target FPGA platform.\n",
    "* `output_dir`: the directory where all the generated build outputs above will be written into.\n",
    "* `steps`: list of predefined (or custom) build steps FINN will go through. Use `build_dataflow_config.estimate_only_dataflow_steps` to execute only the steps needed for estimation (without any synthesis), and the `build_dataflow_config.default_build_dataflow_steps` otherwise (which is the default value). You can find the list of default steps [here](https://finn.readthedocs.io/en/latest/source_code/finn.builder.html#finn.builder.build_dataflow_config.default_build_dataflow_steps) in the documentation.\n",
    "\n",
    "### Configuring the Board and FPGA Part <a id=\"config_fpga\"></a>\n",
    "\n",
    "* `fpga_part`: Xilinx FPGA part to be used for synthesis, can be left unspecified to be inferred from `board` below, or specified explicitly for e.g. out-of-context synthesis.\n",
    "* `board`: target Xilinx Zynq or Alveo board for generating accelerators integrated into a shell. See the `pynq_part_map` and `alveo_part_map` dicts in [this file](https://github.com/Xilinx/finn-base/blob/dev/src/finn/util/basic.py#L41) for a list of possible boards.\n",
    "* `shell_flow_type`: the target [shell flow type](https://finn-dev.readthedocs.io/en/latest/source_code/finn.builder.html#finn.builder.build_dataflow_config.ShellFlowType), only needed for generating full bitfiles where the FINN design is integrated into a shell (so only needed if `BITFILE` is selected) \n",
    "\n",
    "### Configuring the Performance <a id=\"config_perf\"></a>\n",
    "\n",
    "You can configure the performance (and correspondingly, the FPGA resource footprint) of the generated dataflow accelerator in two ways:\n",
    "\n",
    "1) (basic) Set a target performance and let the compiler figure out the per-node parallelization settings.\n",
    "\n",
    "2) (advanced) Specify a separate .json as `folding_config_file` that lists the degree of parallelization (as well as other hardware options) for each layer.\n",
    "\n",
    "This notebook only deals with the basic approach, for which you need to set up:\n",
    "\n",
    "* `target_fps`: target inference performance in frames per second. Note that target may not be achievable due to specific layer constraints, or due to resource limitations of the FPGA. \n",
    "* `synth_clk_period_ns`: target clock frequency (in nanoseconds) for Vivado synthesis. e.g. `synth_clk_period_ns=5.0` will target a 200 MHz clock. Note that the target clock period may not be achievable depending on the FPGA part and design complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b33607b-067c-4cfe-a7c9-cada768aa832",
   "metadata": {},
   "source": [
    "## Launch a Build: Only Estimate Reports <a id=\"build_estimate_report\"></a>\n",
    "\n",
    "##### This is based on \"cybersecurity/3-build-accelerator-with-finn.ipynb\" notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a91174a0-2a8a-47c8-b030-fc101a08b4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous run results deleted!\n"
     ]
    }
   ],
   "source": [
    "import finn.builder.build_dataflow as build\n",
    "import finn.builder.build_dataflow_config as build_cfg\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# model_dir = os.environ['FINN_ROOT'] + \"/notebooks/end2end_example/cybersecurity\"\n",
    "# model_file = model_dir + \"/cybsec-mlp-ready.onnx\"\n",
    "model_file = \"LeNet_WeightAct_tidy.onnx\"\n",
    "\n",
    "estimates_output_dir = \"output_estimates_only\"\n",
    "\n",
    "#Delete previous run results if exist\n",
    "if os.path.exists(estimates_output_dir):\n",
    "    shutil.rmtree(estimates_output_dir)\n",
    "    print(\"Previous run results deleted!\")\n",
    "\n",
    "\n",
    "cfg_estimates = build.DataflowBuildConfig(\n",
    "    output_dir          = estimates_output_dir,\n",
    "    mvau_wwidth_max     = 80,\n",
    "    target_fps          = 1000000,\n",
    "    synth_clk_period_ns = 10.0,\n",
    "    fpga_part           = \"xc7z020clg400-1\",\n",
    "    steps               = build_cfg.estimate_only_dataflow_steps,\n",
    "    generate_outputs=[\n",
    "        build_cfg.DataflowOutputType.ESTIMATE_REPORTS,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d27cc14a-00c0-40cd-8967-7d532bf0d4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataflow accelerator from LeNet_WeightAct_tidy.onnx\n",
      "Intermediate outputs will be generated in /tmp/finn_dev_inets\n",
      "Final outputs will be generated in output_estimates_only\n",
      "Build log is at output_estimates_only/build_dataflow.log\n",
      "Running step: step_qonnx_to_finn [1/10]\n",
      "Running step: step_tidy_up [2/10]\n",
      "Running step: step_streamline [3/10]\n",
      "Running step: step_convert_to_hw [4/10]\n",
      "Running step: step_create_dataflow_partition [5/10]\n",
      "Running step: step_specialize_layers [6/10]\n",
      "Running step: step_target_fps_parallelization [7/10]\n",
      "Running step: step_apply_folding_config [8/10]\n",
      "Running step: step_minimize_bit_width [9/10]\n",
      "Running step: step_generate_estimate_reports [10/10]\n",
      "Completed successfully\n",
      "CPU times: user 1.52 s, sys: 28 ms, total: 1.54 s\n",
      "Wall time: 1.54 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "build.build_dataflow_cfg(model_file, cfg_estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "05410a63-60b0-4656-bf92-6af838186814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert os.path.exists(estimates_output_dir + \"/report/estimate_network_performance.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4ae7711d-f27a-4290-80b9-b379e6ddeb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! ls {estimates_output_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b50e09da-b1ef-4b80-8bd9-1229b2aa8aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! ls {estimates_output_dir}/report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2d36f1db-968d-463e-a77d-85f36b5c5e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! cat {estimates_output_dir}/report/estimate_network_performance.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "541d4c22-5011-4666-921b-5b50fc573de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def read_json_dict(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        ret = json.load(f)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19fe2503-14d9-4c87-be97-189647cf53fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'critical_path_cycles': 12964,\n",
       " 'max_cycles': 2880,\n",
       " 'max_cycles_node_name': 'MVAU_hls_1',\n",
       " 'estimated_throughput_fps': 34722.22222222222,\n",
       " 'estimated_latency_ns': 129640.0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_json_dict(estimates_output_dir + \"/report/estimate_network_performance.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af42ae69-4dba-4696-a77d-5f58dd2f5d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ConvolutionInputGenerator_rtl_0': 1026,\n",
       " 'MVAU_hls_0': 2352,\n",
       " 'StreamingMaxPool_hls_0': 980,\n",
       " 'ConvolutionInputGenerator_rtl_1': 198,\n",
       " 'MVAU_hls_1': 2000,\n",
       " 'StreamingMaxPool_hls_1': 125,\n",
       " 'MVAU_hls_2': 1920,\n",
       " 'MVAU_hls_3': 2016,\n",
       " 'MVAU_hls_4': 840}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_json_dict(estimates_output_dir + \"/report/estimate_layer_cycles.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f523ae52-c8df-4242-94d1-3cca64c2237e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ConvolutionInputGenerator_rtl_0': {},\n",
       " 'MVAU_hls_0': {'op_mac_2bx4b': 352800,\n",
       "  'param_weight_2b': 450,\n",
       "  'param_threshold_10b': 6},\n",
       " 'StreamingMaxPool_hls_0': {},\n",
       " 'ConvolutionInputGenerator_rtl_1': {},\n",
       " 'MVAU_hls_1': {'op_mac_2bx2b': 240000,\n",
       "  'param_weight_2b': 2400,\n",
       "  'param_threshold_9b': 16},\n",
       " 'StreamingMaxPool_hls_1': {},\n",
       " 'MVAU_hls_2': {'op_mac_2bx2b': 48000,\n",
       "  'param_weight_2b': 48000,\n",
       "  'param_threshold_8b': 120},\n",
       " 'MVAU_hls_3': {'op_mac_2bx2b': 10080,\n",
       "  'param_weight_2b': 10080,\n",
       "  'param_threshold_7b': 84},\n",
       " 'MVAU_hls_4': {'op_mac_2bx8b': 840, 'param_weight_8b': 840},\n",
       " 'total': {'op_mac_2bx4b': 352800.0,\n",
       "  'param_weight_2b': 60930.0,\n",
       "  'param_threshold_10b': 6.0,\n",
       "  'op_mac_2bx2b': 298080.0,\n",
       "  'param_threshold_9b': 16.0,\n",
       "  'param_threshold_8b': 120.0,\n",
       "  'param_threshold_7b': 84.0,\n",
       "  'op_mac_2bx8b': 840.0,\n",
       "  'param_weight_8b': 840.0}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_json_dict(estimates_output_dir + \"/report/op_and_param_counts.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1dc3e11-dd18-4512-8961-4fe2144bac6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ConvolutionInputGenerator_rtl_0': {'BRAM_18K': 0,\n",
       "  'BRAM_efficiency': 1,\n",
       "  'LUT': 348,\n",
       "  'URAM': 0,\n",
       "  'URAM_efficiency': 1,\n",
       "  'DSP': 0},\n",
       " 'MVAU_hls_0': {'BRAM_18K': 9,\n",
       "  'BRAM_efficiency': 0.005425347222222222,\n",
       "  'LUT': 3296,\n",
       "  'URAM': 0,\n",
       "  'URAM_efficiency': 1,\n",
       "  'DSP': 0},\n",
       " 'ConvolutionInputGenerator_rtl_1': {'BRAM_18K': 0,\n",
       "  'BRAM_efficiency': 1,\n",
       "  'LUT': 348,\n",
       "  'URAM': 0,\n",
       "  'URAM_efficiency': 1,\n",
       "  'DSP': 0},\n",
       " 'MVAU_hls_1': {'BRAM_18K': 27,\n",
       "  'BRAM_efficiency': 0.009645061728395061,\n",
       "  'LUT': 6724,\n",
       "  'URAM': 0,\n",
       "  'URAM_efficiency': 1,\n",
       "  'DSP': 0},\n",
       " 'MVAU_hls_2': {'BRAM_18K': 144,\n",
       "  'BRAM_efficiency': 0.8333333333333334,\n",
       "  'LUT': 6094,\n",
       "  'URAM': 0,\n",
       "  'URAM_efficiency': 1,\n",
       "  'DSP': 0},\n",
       " 'MVAU_hls_3': {'BRAM_18K': 2,\n",
       "  'BRAM_efficiency': 0.546875,\n",
       "  'LUT': 357,\n",
       "  'URAM': 0,\n",
       "  'URAM_efficiency': 1,\n",
       "  'DSP': 0},\n",
       " 'MVAU_hls_4': {'BRAM_18K': 1,\n",
       "  'BRAM_efficiency': 0.3645833333333333,\n",
       "  'LUT': 361,\n",
       "  'URAM': 0,\n",
       "  'URAM_efficiency': 1,\n",
       "  'DSP': 0},\n",
       " 'total': {'BRAM_18K': 183.0, 'LUT': 17528.0, 'URAM': 0.0, 'DSP': 0.0}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_json_dict(estimates_output_dir + \"/report/estimate_layer_resources.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a33f8c97-6693-4d97-93a4-77c0048ac623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_json_dict(estimates_output_dir + \"/report/estimate_layer_config_alternatives.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d8690d-4932-4d0c-b80a-916fd77eb451",
   "metadata": {},
   "source": [
    "## Launch a Build: Stitched IP, out-of-context synth and rtlsim Performance <a id=\"build_ip_synth_rtlsim\"></a>\n",
    "\n",
    "Once we have a configuration that gives satisfactory estimates, we can move on to generating the accelerator. We can do this in different ways depending on how we want to integrate the accelerator into a larger system. For instance, if we have a larger streaming system built in Vivado or if we'd like to re-use this generated accelerator as an IP component in other projects, the `STITCHED_IP` output product is a good choice. We can also use the `OOC_SYNTH` output product to get post-synthesis resource and clock frequency numbers for our accelerator.\n",
    "\n",
    "<font color=\"red\">**Live FINN tutorial:** These next builds will take about 10 minutes to complete since multiple calls to Vivado and a call to RTL simulation are involved. While this is running, you can examine the generated files with noVNC -- it is running on **(your AWS URL):6080/vnc.html**\n",
    "\n",
    "* Once the `step_hls_codegen [8/16]` below is completed, you can view the generated HLS code under its own folder for each layer: `/tmp/finn_dev_ubuntu/code_gen_ipgen_MVAU_hls_XXXXXX`\n",
    "    \n",
    "* Once the `step_create_stitched_ip [11/16]` below is completed, you can view the generated stitched IP in Vivado under `/home/ubuntu/finn/notebooks/end2end_example/cybersecurity/output_ipstitch_ooc_rtlsim/stitched_ip`\n",
    "</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4a5a9aeb-faf8-4f5e-a9d1-51442cb28c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous run results deleted!\n"
     ]
    }
   ],
   "source": [
    "import finn.builder.build_dataflow as build\n",
    "import finn.builder.build_dataflow_config as build_cfg\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "model_file = \"LeNet_WeightAct_tidy.onnx\"\n",
    "# model_file = \"LeNet_WeightAct_dataflow_model.onnx\"\n",
    "\n",
    "rtlsim_output_dir = \"output_ipstitch_ooc_rtlsim\"\n",
    "\n",
    "#Delete previous run results if exist\n",
    "if os.path.exists(rtlsim_output_dir):\n",
    "    shutil.rmtree(rtlsim_output_dir)\n",
    "    print(\"Previous run results deleted!\")\n",
    "\n",
    "cfg_stitched_ip = build.DataflowBuildConfig(\n",
    "    output_dir          = rtlsim_output_dir,\n",
    "    mvau_wwidth_max     = 15,   # 10 works, 15, 20 not.\n",
    "    target_fps          = 1000000,\n",
    "    synth_clk_period_ns = 10.0,\n",
    "    fpga_part           = \"xc7z020clg400-1\",\n",
    "    # specialize_layers_config_file=\"specialize_layers_config.json\",\n",
    "    generate_outputs=[\n",
    "        build_cfg.DataflowOutputType.STITCHED_IP,\n",
    "        build_cfg.DataflowOutputType.RTLSIM_PERFORMANCE,\n",
    "        build_cfg.DataflowOutputType.OOC_SYNTH,\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4a258c46-14ee-421f-96a1-e6a8c0c10687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataflow accelerator from LeNet_WeightAct_tidy.onnx\n",
      "Intermediate outputs will be generated in /tmp/finn_dev_inets\n",
      "Final outputs will be generated in output_ipstitch_ooc_rtlsim\n",
      "Build log is at output_ipstitch_ooc_rtlsim/build_dataflow.log\n",
      "Running step: step_qonnx_to_finn [1/19]\n",
      "Running step: step_tidy_up [2/19]\n",
      "Running step: step_streamline [3/19]\n",
      "Running step: step_convert_to_hw [4/19]\n",
      "Running step: step_create_dataflow_partition [5/19]\n",
      "Running step: step_specialize_layers [6/19]\n",
      "Running step: step_target_fps_parallelization [7/19]\n",
      "Running step: step_apply_folding_config [8/19]\n",
      "Running step: step_minimize_bit_width [9/19]\n",
      "Running step: step_generate_estimate_reports [10/19]\n",
      "Running step: step_hw_codegen [11/19]\n",
      "Running step: step_hw_ipgen [12/19]\n",
      "Running step: step_set_fifo_depths [13/19]\n",
      "Running step: step_create_stitched_ip [14/19]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/file_util.py\", line 40, in _copy_file_contents\n",
      "    fdst = open(dst, 'wb')\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'output_ipstitch_ooc_rtlsim/stitched_ip/make_project.tcl'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/inets/arish/FINN_arish/finn/src/finn/builder/build_dataflow.py\", line 158, in build_dataflow_cfg\n",
      "    model = transform_step(model, cfg)\n",
      "  File \"/home/inets/arish/FINN_arish/finn/src/finn/builder/build_dataflow_steps.py\", line 652, in step_create_stitched_ip\n",
      "    copy_tree(model.get_metadata_prop(\"vivado_stitch_proj\"), stitched_ip_dir)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/dir_util.py\", line 185, in copy_tree\n",
      "    copy_file(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/file_util.py\", line 163, in copy_file\n",
      "    _copy_file_contents(src, dst)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/file_util.py\", line 42, in _copy_file_contents\n",
      "    raise DistutilsFileError(\n",
      "distutils.errors.DistutilsFileError: could not create 'output_ipstitch_ooc_rtlsim/stitched_ip/make_project.tcl': No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/file_util.py\u001b[0m(42)\u001b[0;36m_copy_file_contents\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     40 \u001b[0;31m            \u001b[0mfdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     41 \u001b[0;31m        \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 42 \u001b[0;31m            raise DistutilsFileError(\n",
      "\u001b[0m\u001b[0;32m     43 \u001b[0;31m                \u001b[0;34m\"could not create '{}': {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrerror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     44 \u001b[0;31m            )\n",
      "\u001b[0m\n",
      "--KeyboardInterrupt--\n",
      "\n",
      "KeyboardInterrupt: Interrupted by user\n",
      "Build failed\n",
      "CPU times: user 8.35 s, sys: 224 ms, total: 8.57 s\n",
      "Wall time: 9min 50s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "build.build_dataflow_cfg(model_file, cfg_stitched_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "06bd2ab4-b535-4619-9207-1a8134876a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.exists(rtlsim_output_dir + \"/report/ooc_synth_and_timing.json\")\n",
    "assert os.path.exists(rtlsim_output_dir + \"/report/rtlsim_performance.json\")\n",
    "assert os.path.exists(rtlsim_output_dir + \"/final_hw_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2bb9396e-0253-4b96-b80a-c9a54a6104f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_verilog_srcs.txt\t\t       finn_vivado_stitch_proj.xpr\n",
      "data\t\t\t\t       ip\n",
      "finn_vivado_stitch_proj.cache\t       make_project.sh\n",
      "finn_vivado_stitch_proj.gen\t       make_project.tcl\n",
      "finn_vivado_stitch_proj.hw\t       vivado.jou\n",
      "finn_vivado_stitch_proj.ip_user_files  vivado.log\n",
      "finn_vivado_stitch_proj.srcs\n"
     ]
    }
   ],
   "source": [
    "! ls {rtlsim_output_dir}/stitched_ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ab45333d-9903-481e-bf2b-73655534223b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimate_layer_resources_hls.json  rtlsim_performance.json\n",
      "ooc_synth_and_timing.json\n"
     ]
    }
   ],
   "source": [
    "! ls {rtlsim_output_dir}/report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c67ca0af-75b1-4ec8-9846-fe01cc8df889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"vivado_proj_folder\": \"/tmp/finn_dev_inets/synth_out_of_context__jv70oh0/results_finn_design_wrapper\",\n",
      "  \"LUT\": 6801.0,\n",
      "  \"LUTRAM\": 540.0,\n",
      "  \"FF\": 10016.0,\n",
      "  \"DSP\": 0.0,\n",
      "  \"BRAM\": 61.0,\n",
      "  \"BRAM_18K\": 30.0,\n",
      "  \"BRAM_36K\": 46.0,\n",
      "  \"URAM\": 0.0,\n",
      "  \"Carry\": 535.0,\n",
      "  \"WNS\": 0.735,\n",
      "  \"Delay\": 0.735,\n",
      "  \"vivado_version\": 2022.2,\n",
      "  \"vivado_build_no\": 3671981.0,\n",
      "  \"\": 0,\n",
      "  \"fmax_mhz\": 107.93308148947652,\n",
      "  \"estimated_throughput_fps\": 1835.596623970689\n",
      "}"
     ]
    }
   ],
   "source": [
    "! cat {rtlsim_output_dir}/report/ooc_synth_and_timing.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8bfcf9cc-f881-4b3a-88d5-f2d41d09271b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"N_IN_TXNS\": 1024,\n",
      "  \"N_OUT_TXNS\": 10,\n",
      "  \"cycles\": 120210,\n",
      "  \"N\": 1,\n",
      "  \"latency_cycles\": 120210,\n",
      "  \"runtime[ms]\": 1.2021,\n",
      "  \"throughput[images/s]\": 831.8775476249896,\n",
      "  \"fclk[mhz]\": 100.0,\n",
      "  \"stable_throughput[images/s]\": 831.8775476249896\n",
      "}"
     ]
    }
   ],
   "source": [
    "! cat {rtlsim_output_dir}/report/rtlsim_performance.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7e335d81-c1bb-45cf-bac9-a5ffa4e8807f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Defaults\": {},\n",
      "  \"StreamingFIFO_rtl_0\": {\n",
      "    \"ram_style\": \"auto\",\n",
      "    \"depth\": 1024,\n",
      "    \"impl_style\": \"rtl\",\n",
      "    \"inFIFODepths\": [\n",
      "      0\n",
      "    ],\n",
      "    \"outFIFODepths\": [\n",
      "      0\n",
      "    ]\n",
      "  },\n",
      "  \"ConvolutionInputGenerator_rtl_0\": {\n",
      "    \"SIMD\": 3,\n",
      "    \"parallel_window\": 0,\n",
      "    \"ram_style\": \"distributed\",\n",
      "    \"inFIFODepths\": [\n",
      "      1024\n",
      "    ],\n",
      "    \"outFIFODepths\": [\n",
      "      19600\n",
      "    ]\n",
      "  },\n",
      "  \"StreamingFIFO_rtl_1\": {\n",
      "    \"ram_style\": \"auto\",\n",
      "    \"depth\": 19600,\n",
      "    \"impl_style\": \"vivado\",\n",
      "    \"inFIFODepths\": [\n",
      "      0\n",
      "    ],\n",
      "    \"outFIFODepths\": [\n",
      "      0\n",
      "    ]\n",
      "  },\n",
      "  \"StreamingDataWidthConverter_rtl_0\": {\n",
      "    \"inFIFODepths\": [\n",
      "      19600\n",
      "    ],\n",
      "    \"outFIFODepths\": [\n",
      "      32\n",
      "    ]\n",
      "  },\n",
      "  \"StreamingFIFO_rtl_2\": {\n",
      "    \"ram_style\": \"auto\",\n",
      "    \"depth\": 32,\n",
      "    \"impl_style\": \"rtl\",\n",
      "    \"inFIFODepths\": [\n",
      "      0\n",
      "    ],\n",
      "    \"outFIFODepths\": [\n",
      "      0\n",
      "    ]\n",
      "  },\n",
      "  \"MVAU_hls_0\": {\n",
      "    \"PE\": 6,\n",
      "    \"SIMD\": 1,\n",
      "    \"ram_style\": \"auto\",\n",
      "    \"resType\": \"auto\",\n",
      "    \"mem_mode\": \"internal_decoupled\",\n",
      "    \"runtime_writeable_weights\": 0,\n",
      "    \"inFIFODepths\": [\n",
      "      32\n",
      "    ],\n",
      "    \"outFIFODepths\": [\n",
      "      2\n",
      "    ]\n",
      "  },\n",
      "  \"StreamingFIFO_rtl_3\": {\n",
      "    \"ram_style\": \"auto\",\n",
      "    \"depth\": 2,\n",
      "    \"impl_style\": \"rtl\",\n",
      "    \"inFIFODepths\": [\n",
      "      0\n",
      "    ],\n",
      "    \"outFIFODepths\": [\n",
      "      0\n",
      "    ]\n",
      "  },\n",
      "  \"StreamingMaxPool_hls_0\": {\n",
      "    \"PE\": 1,\n",
      "    \"inFIFODepths\": [\n",
      "      2\n",
      "    ],\n",
      "    \"outFIFODepths\": [\n",
      "      32\n",
      "    ]\n",
      "  },\n",
      "  \"StreamingFIFO_rtl_4\": {\n",
      "    \"ram_style\": \"auto\",\n",
      "    \"depth\": 32,\n",
      "    \"impl_style\": \"rtl\",\n",
      "    \"inFIFODepths\": [\n",
      "      0\n",
      "    ],\n",
      "    \"outFIFODepths\": [\n",
      "      0\n",
      "    ]\n",
      "  },\n",
      "  \"StreamingDataWidthConverter_rtl_1\": {\n",
      "    \"inFIFODepths\": [\n",
      "      32\n",
      "    ],\n",
      "    \"outFIFODepths\": [\n",
      "      42\n",
      "    ]\n",
      "  },\n",
      "  \"StreamingFIFO_rtl_5\": {\n",
      "    \"ram_style\": \"auto\",\n",
      "    \"depth\": 42,\n",
      "    \"impl_style\": \"rtl\",\n",
      "    \"inFIFODepths\": [\n",
      "      0\n",
      "    ],\n",
      "    \"outFIFODepths\": [\n",
      "      0\n",
      "    ]\n",
      "  },\n",
      "  \"ConvolutionInputGenerator_rtl_1\": {\n",
      "    \"SIMD\": 1,\n",
      "    \"parallel_window\": 0,\n",
      "    \"ram_style\": \"distributed\",\n",
      "    \"inFIFODepths\": [\n",
      "      42\n",
      "    ],\n",
      "    \"outFIFODepths\": [\n",
      "      2\n",
      "    ]\n",
      "  },\n",
      "  \"StreamingFIFO_rtl_6\": {\n",
      "    \"ram_style\": \"auto\",\n",
      "    \"depth\": 2,\n",
      "    \"impl_style\": \"rtl\",\n",
      "    \"inFIFODepths\": [\n",
      "      0\n",
      "    ],\n",
      "    \"outFIFODepths\": [\n",
      "      0\n",
      "    ]\n",
      "  },\n",
      "  \"StreamingDataWidthConverter_rtl_2\": {\n",
      "    \"inFIFODepths\": [\n",
      "      2\n",
      "    ],\n",
      "    \"outFIFODepths\": [\n",
      "      412\n",
      "    ]\n",
      "  },\n",
      "  \"StreamingFIFO_rtl_7\": {\n",
      "    \"ram_style\": \"auto\",\n",
      "    \"depth\": 412,\n",
      "    \"impl_style\": \"vivado\",\n",
      "    \"inFIFODepths\": [\n",
      "      0\n",
      "    ],\n",
      "    \"outFIFODepths\": [\n",
      "      0\n",
      "    ]\n",
      "  },\n",
      "  \"MVAU_hls_1\": {\n",
      "    \"PE\": 4,\n",
      "    \"SIMD\": 2,\n",
      "    \"ram_style\": \"auto\",\n",
      "    \"resType\": \"auto\",\n",
      "    \"mem_mode\": \"internal_decoupled\",\n",
      "    \"runtime_writeable_weights\": 0,\n",
      "    \"inFIFODepths\": [\n",
      "      412\n",
      "    ],\n",
      "    \"outFIFODepths\": [\n",
      "      280\n",
      "    ]\n",
      "  },\n",
      "  \"StreamingFIFO_rtl_8\": {\n",
      "    \"ram_style\": \"auto\",\n",
      "    \"depth\": 280,\n",
      "    \"impl_style\": \"vivado\",\n",
      "    \"inFIFODepths\": [\n",
      "      0\n",
      "    ],\n",
      "    \"outFIFODepths\": [\n",
      "      0\n",
      "    ]\n",
      "  },\n",
      "  \"MVAU_hls_2\": {\n",
      "    \"PE\": 1,\n",
      "    \"SIMD\": 4,\n",
      "    \"ram_style\": \"auto\",\n",
      "    \"resType\": \"auto\",\n",
      "    \"mem_mode\": \"internal_decoupled\",\n",
      "    \"runtime_writeable_weights\": 0,\n",
      "    \"inFIFODepths\": [\n",
      "      280\n",
      "    ],\n",
      "    \"outFIFODepths\": [\n",
      "      2\n",
      "    ]\n",
      "  },\n",
      "  \"StreamingFIFO_rtl_9\": {\n",
      "    \"ram_style\": \"auto\",\n",
      "    \"depth\": 2,\n",
      "    \"impl_style\": \"rtl\",\n",
      "    \"inFIFODepths\": [\n",
      "      0\n",
      "    ],\n",
      "    \"outFIFODepths\": [\n",
      "      0\n",
      "    ]\n",
      "  },\n",
      "  \"MVAU_hls_3\": {\n",
      "    \"PE\": 1,\n",
      "    \"SIMD\": 1,\n",
      "    \"ram_style\": \"auto\",\n",
      "    \"resType\": \"auto\",\n",
      "    \"mem_mode\": \"internal_decoupled\",\n",
      "    \"runtime_writeable_weights\": 0,\n",
      "    \"inFIFODepths\": [\n",
      "      2\n",
      "    ],\n",
      "    \"outFIFODepths\": [\n",
      "      2\n",
      "    ]\n",
      "  },\n",
      "  \"StreamingFIFO_rtl_10\": {\n",
      "    \"ram_style\": \"auto\",\n",
      "    \"depth\": 2,\n",
      "    \"impl_style\": \"rtl\",\n",
      "    \"inFIFODepths\": [\n",
      "      0\n",
      "    ],\n",
      "    \"outFIFODepths\": [\n",
      "      0\n",
      "    ]\n",
      "  },\n",
      "  \"MVAU_rtl_0\": {\n",
      "    \"PE\": 1,\n",
      "    \"SIMD\": 1,\n",
      "    \"ram_style\": \"auto\",\n",
      "    \"resType\": \"auto\",\n",
      "    \"mem_mode\": \"internal_decoupled\",\n",
      "    \"runtime_writeable_weights\": 0,\n",
      "    \"inFIFODepths\": [\n",
      "      2\n",
      "    ],\n",
      "    \"outFIFODepths\": [\n",
      "      2\n",
      "    ]\n",
      "  },\n",
      "  \"StreamingFIFO_rtl_11\": {\n",
      "    \"ram_style\": \"auto\",\n",
      "    \"depth\": 2,\n",
      "    \"impl_style\": \"rtl\",\n",
      "    \"inFIFODepths\": [\n",
      "      0\n",
      "    ],\n",
      "    \"outFIFODepths\": [\n",
      "      0\n",
      "    ]\n",
      "  }\n",
      "}"
     ]
    }
   ],
   "source": [
    "! cat {rtlsim_output_dir}/final_hw_config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6064a0-8bd7-472b-9c32-8f07358e7b91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
