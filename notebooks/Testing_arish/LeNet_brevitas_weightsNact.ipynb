{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fc4fbc6-69d3-4dad-a800-83dba289d582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target device: cpu\n"
     ]
    }
   ],
   "source": [
    "# https://xilinx.github.io/brevitas/getting_started.html\n",
    "\n",
    "from torch.nn import Module\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import brevitas.nn as qnn\n",
    "from brevitas.quant import Int8Bias as BiasQuant\n",
    "\n",
    "import os\n",
    "import onnx\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from brevitas.nn import QuantLinear, QuantReLU\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "weight_bit_width = 4\n",
    "act_bit_width = 4\n",
    "\n",
    "class QuantWeightActLeNet(Module):\n",
    "    def __init__(self):\n",
    "        super(QuantWeightActLeNet, self).__init__()\n",
    "        self.quant_inp = qnn.QuantIdentity(bit_width=4)\n",
    "        self.conv1 = qnn.QuantConv2d(3, 6, 5, bias=True, weight_bit_width=weight_bit_width)\n",
    "        self.relu1 = qnn.QuantReLU(bit_width=act_bit_width)\n",
    "        self.conv2 = qnn.QuantConv2d(6, 16, 5, bias=True, weight_bit_width=weight_bit_width)\n",
    "        self.relu2 = qnn.QuantReLU(bit_width=act_bit_width)\n",
    "        self.fc1   = qnn.QuantLinear(16*5*5, 120, bias=True, weight_bit_width=weight_bit_width)\n",
    "        self.relu3 = qnn.QuantReLU(bit_width=act_bit_width)\n",
    "        self.fc2   = qnn.QuantLinear(120, 84, bias=True, weight_bit_width=weight_bit_width)\n",
    "        self.relu4 = qnn.QuantReLU(bit_width=act_bit_width)\n",
    "        self.fc3   = qnn.QuantLinear(84, 10, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.quant_inp(x)\n",
    "        out = self.relu1(self.conv1(out))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = self.relu2(self.conv2(out))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.relu3(self.fc1(out))\n",
    "        out = self.relu4(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "model = QuantWeightActLeNet()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Target device: \" + str(device))\n",
    "\n",
    "model.to(device); # The semicolon is for not printing the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ad4f53-4e45-4217-97f3-678171345246",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59b7b02a-8bb0-4dd3-b5e4-accf4aaa6f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Saved the dataset as .npz files\n"
     ]
    }
   ],
   "source": [
    "# This is for 4-bit quantization. The quantization value can be changed in line 11. \n",
    "\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "\n",
    "quant_param = 16 # log2(quant_param) bits. #256 means no quanization, 1 means 1-bit quantization\n",
    "\n",
    "def quantize_image(image):\n",
    "    \"\"\"Quantize and binarize an image.\"\"\"\n",
    "    image = image.astype(np.float32)\n",
    "    image = np.floor(image / (256/quant_param))  # Example: reducing to 4-bit quantization\n",
    "    return image.astype(np.float32)\n",
    "\n",
    "def save_dataset_as_npz(data, labels, filename):\n",
    "    \"\"\"Save the dataset as a .npz file.\"\"\"\n",
    "    np.savez_compressed(filename, data=data, labels=labels)\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Quantize the images\n",
    "train_images = np.array([quantize_image(image.numpy().transpose(1, 2, 0) * 255) for image, _ in train_dataset])\n",
    "train_labels = np.array(train_dataset.targets)\n",
    "\n",
    "test_images = np.array([quantize_image(image.numpy().transpose(1, 2, 0) * 255) for image, _ in test_dataset])\n",
    "test_labels = np.array(test_dataset.targets)\n",
    "\n",
    "# Save the datasets\n",
    "os.makedirs('./quantized_data', exist_ok=True)\n",
    "save_dataset_as_npz(train_images, train_labels, './quantized_data/cifar10_train.npz')\n",
    "save_dataset_as_npz(test_images, test_labels, './quantized_data/cifar10_test.npz')\n",
    "\n",
    "print('Saved the dataset as .npz files')\n",
    "\n",
    "\n",
    "class CIFAR10QuantizedDataset(Dataset):\n",
    "    def __init__(self, npz_file):\n",
    "        data = np.load(npz_file)\n",
    "        self.images = data['data']\n",
    "        self.labels = data['labels']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx].astype(np.float32) / 255.0\n",
    "        label = self.labels[idx]\n",
    "        image = torch.tensor(image.transpose(2, 0, 1))  # HWC to CHW format\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return image, label\n",
    "\n",
    "# Load the quantized dataset\n",
    "train_dataset = CIFAR10QuantizedDataset('./quantized_data/cifar10_train.npz')\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "test_dataset = CIFAR10QuantizedDataset('./quantized_data/cifar10_test.npz')\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d13af9-200e-4f88-9f38-5f35d18bb9ce",
   "metadata": {},
   "source": [
    "### Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ba85b47-0263-4bdf-a0b0-7e76ce4dc7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss = 1.303191 test accuracy = 0.516700: 100%|â–ˆ| 10/10 [01:59<00:00, 1\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train(model, train_loader, optimizer, criterion):\n",
    "    losses = []\n",
    "    # ensure model is in training mode\n",
    "    model.train()    \n",
    "    \n",
    "    for i, data in enumerate(train_loader, 0):        \n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()   \n",
    "                \n",
    "        # forward pass\n",
    "        output = model(images.float())\n",
    "        # loss = criterion(output, labels.unsqueeze(1))\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        # backward pass + run optimizer to update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # keep track of loss value\n",
    "        losses.append(loss.data.cpu().numpy()) \n",
    "    return losses\n",
    "\n",
    "\n",
    "def test(model, test_loader):    \n",
    "    # ensure model is in eval mode\n",
    "    model.eval() \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "   \n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            output = model(images.float())\n",
    "            # run the output through sigmoid\n",
    "            # output = torch.sigmoid(output_orig)  \n",
    "            # compare against a threshold of 0.5 to generate 0/1\n",
    "            # pred = (output.detach().cpu().numpy() > 0.5) * 1\n",
    "            _, pred = torch.max(output.data, 1)\n",
    "            labels = labels.cpu().float()\n",
    "            y_true.extend(labels.tolist()) \n",
    "            y_pred.extend(pred.reshape(-1).tolist())\n",
    "            # y_pred.extend((pred == labels).sum().item())\n",
    "        \n",
    "    return accuracy_score(y_true, y_pred)\n",
    "\n",
    "num_epochs = 10\n",
    "lr = 0.001 \n",
    "\n",
    "def display_loss_plot(losses, title=\"Training loss\", xlabel=\"Iterations\", ylabel=\"Loss\"):\n",
    "    x_axis = [i for i in range(len(losses))]\n",
    "    plt.plot(x_axis,losses)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()\n",
    "\n",
    "# loss criterion and optimizer\n",
    "# criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Setting seeds for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "running_loss = []\n",
    "running_test_acc = []\n",
    "t = trange(num_epochs, desc=\"Training loss\", leave=True)\n",
    "\n",
    "for epoch in t:\n",
    "        loss_epoch = train(model, train_loader, optimizer,criterion)\n",
    "        test_acc = test(model, test_loader)\n",
    "        t.set_description(\"Training loss = %f test accuracy = %f\" % (np.mean(loss_epoch), test_acc))\n",
    "        t.refresh() # to show immediately the update           \n",
    "        running_loss.append(loss_epoch)\n",
    "        running_test_acc.append(test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d0ba76-1444-4d26-a305-272c102f909c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3e05908-29dc-4f6b-b3d4-67817d90be98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy =  0.5167\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = test(model, test_loader)\n",
    "print('test accuracy = ', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc3c92a8-51cc-48ab-b7dc-f253413e18ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Brevitas model to disk\n",
    "torch.save(model.state_dict(), \"state_dict_LeNet_WeightAct.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72396db9-ef9b-4f17-bfda-3e5c6e26184c",
   "metadata": {},
   "source": [
    "####  Convert to ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864c180c-c4d7-496f-85f1-b11d9e74dbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from brevitas.export import export_onnx_qcdq\n",
    "import torch\n",
    "\n",
    "# Weight-activation model\n",
    "export_onnx_qcdq(quant_weight_act_lenet, torch.randn(1, 3, 32, 32), export_path='4b_weight_act_lenet.onnx')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
