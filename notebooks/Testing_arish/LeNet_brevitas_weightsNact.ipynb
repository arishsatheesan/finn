{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5fc4fbc6-69d3-4dad-a800-83dba289d582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target device: cpu\n"
     ]
    }
   ],
   "source": [
    "# https://xilinx.github.io/brevitas/getting_started.html\n",
    "\n",
    "from torch.nn import Module\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import brevitas.nn as qnn\n",
    "from brevitas.quant import Int8Bias as BiasQuant\n",
    "\n",
    "import os\n",
    "import onnx\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from brevitas.nn import QuantLinear, QuantReLU\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "weight_bit_width = 2\n",
    "act_bit_width = 2\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "class QuantWeightActLeNet(Module):\n",
    "    def __init__(self):\n",
    "        super(QuantWeightActLeNet, self).__init__()\n",
    "        self.quant_inp = qnn.QuantIdentity(bit_width=4)\n",
    "        self.conv1 = qnn.QuantConv2d(3, 6, 5, bias=True, weight_bit_width=weight_bit_width)\n",
    "        self.relu1 = qnn.QuantReLU(bit_width=act_bit_width)\n",
    "        self.conv2 = qnn.QuantConv2d(6, 16, 5, bias=True, weight_bit_width=weight_bit_width)\n",
    "        self.relu2 = qnn.QuantReLU(bit_width=act_bit_width)\n",
    "        self.fc1   = qnn.QuantLinear(16*5*5, 120, bias=True, weight_bit_width=weight_bit_width)\n",
    "        self.relu3 = qnn.QuantReLU(bit_width=act_bit_width)\n",
    "        self.fc2   = qnn.QuantLinear(120, 84, bias=True, weight_bit_width=weight_bit_width)\n",
    "        self.relu4 = qnn.QuantReLU(bit_width=act_bit_width)\n",
    "        self.fc3   = qnn.QuantLinear(84, 10, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.quant_inp(x)\n",
    "        out = self.relu1(self.conv1(out))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = self.relu2(self.conv2(out))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.relu3(self.fc1(out))\n",
    "        out = self.relu4(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "model = QuantWeightActLeNet()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Target device: \" + str(device))\n",
    "\n",
    "model.to(device); # The semicolon is for not printing the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ad4f53-4e45-4217-97f3-678171345246",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "59b7b02a-8bb0-4dd3-b5e4-accf4aaa6f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Saved the dataset as .npz files\n"
     ]
    }
   ],
   "source": [
    "# This is for 4-bit quantization. The quantization value can be changed in line 11. \n",
    "\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "\n",
    "quant_param = 4 # log2(quant_param) bits. #256 means no quanization, 2 means 1-bit quantization\n",
    "\n",
    "def quantize_image(image):\n",
    "    \"\"\"Quantize and binarize an image.\"\"\"\n",
    "    image = image.astype(np.float32)\n",
    "    image = np.floor(image / (256/quant_param))  # Example: reducing to 4-bit quantization\n",
    "    return image.astype(np.float32)\n",
    "\n",
    "def save_dataset_as_npz(data, labels, filename):\n",
    "    \"\"\"Save the dataset as a .npz file.\"\"\"\n",
    "    np.savez_compressed(filename, data=data, labels=labels)\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Quantize the images\n",
    "train_images = np.array([quantize_image(image.numpy().transpose(1, 2, 0) * 255) for image, _ in train_dataset])\n",
    "train_labels = np.array(train_dataset.targets)\n",
    "\n",
    "test_images = np.array([quantize_image(image.numpy().transpose(1, 2, 0) * 255) for image, _ in test_dataset])\n",
    "test_labels = np.array(test_dataset.targets)\n",
    "\n",
    "# Save the datasets\n",
    "os.makedirs('./quantized_data', exist_ok=True)\n",
    "save_dataset_as_npz(train_images, train_labels, './quantized_data/cifar10_train.npz')\n",
    "save_dataset_as_npz(test_images, test_labels, './quantized_data/cifar10_test.npz')\n",
    "\n",
    "print('Saved the dataset as .npz files')\n",
    "\n",
    "\n",
    "class CIFAR10QuantizedDataset(Dataset):\n",
    "    def __init__(self, npz_file):\n",
    "        data = np.load(npz_file)\n",
    "        self.images = data['data']\n",
    "        self.labels = data['labels']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx].astype(np.float32) / 255.0\n",
    "        label = self.labels[idx]\n",
    "        image = torch.tensor(image.transpose(2, 0, 1))  # HWC to CHW format\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return image, label\n",
    "\n",
    "# Load the quantized dataset\n",
    "train_dataset = CIFAR10QuantizedDataset('./quantized_data/cifar10_train.npz')\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "test_dataset = CIFAR10QuantizedDataset('./quantized_data/cifar10_test.npz')\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d13af9-200e-4f88-9f38-5f35d18bb9ce",
   "metadata": {},
   "source": [
    "### Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5ba85b47-0263-4bdf-a0b0-7e76ce4dc7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss = 1.666302 test accuracy = 0.387600: 100%|â–ˆ| 10/10 [00:59<00:00,  \n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train(model, train_loader, optimizer, criterion):\n",
    "    losses = []\n",
    "    # ensure model is in training mode\n",
    "    model.train()    \n",
    "    \n",
    "    for i, data in enumerate(train_loader, 0):        \n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()   \n",
    "                \n",
    "        # forward pass\n",
    "        output = model(images.float())\n",
    "        # loss = criterion(output, labels.unsqueeze(1))\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        # backward pass + run optimizer to update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # keep track of loss value\n",
    "        losses.append(loss.data.cpu().numpy()) \n",
    "    return losses\n",
    "\n",
    "\n",
    "def test(model, test_loader):    \n",
    "    # ensure model is in eval mode\n",
    "    model.eval() \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "   \n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            output = model(images.float())\n",
    "            # run the output through sigmoid\n",
    "            # output = torch.sigmoid(output_orig)  \n",
    "            # compare against a threshold of 0.5 to generate 0/1\n",
    "            # pred = (output.detach().cpu().numpy() > 0.5) * 1\n",
    "            _, pred = torch.max(output.data, 1)\n",
    "            labels = labels.cpu().float()\n",
    "            y_true.extend(labels.tolist()) \n",
    "            y_pred.extend(pred.reshape(-1).tolist())\n",
    "            # y_pred.extend((pred == labels).sum().item())\n",
    "        \n",
    "    return accuracy_score(y_true, y_pred)\n",
    "\n",
    "num_epochs = 10\n",
    "lr = 0.001 \n",
    "\n",
    "def display_loss_plot(losses, title=\"Training loss\", xlabel=\"Iterations\", ylabel=\"Loss\"):\n",
    "    x_axis = [i for i in range(len(losses))]\n",
    "    plt.plot(x_axis,losses)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()\n",
    "\n",
    "# loss criterion and optimizer\n",
    "# criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Setting seeds for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "running_loss = []\n",
    "running_test_acc = []\n",
    "t = trange(num_epochs, desc=\"Training loss\", leave=True)\n",
    "\n",
    "for epoch in t:\n",
    "        loss_epoch = train(model, train_loader, optimizer,criterion)\n",
    "        test_acc = test(model, test_loader)\n",
    "        t.set_description(\"Training loss = %f test accuracy = %f\" % (np.mean(loss_epoch), test_acc))\n",
    "        t.refresh() # to show immediately the update           \n",
    "        running_loss.append(loss_epoch)\n",
    "        running_test_acc.append(test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d0ba76-1444-4d26-a305-272c102f909c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b3e05908-29dc-4f6b-b3d4-67817d90be98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy =  0.3876\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = test(model, test_loader)\n",
    "print('test accuracy = ', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cc3c92a8-51cc-48ab-b7dc-f253413e18ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Brevitas model to disk\n",
    "torch.save(model.state_dict(), \"state_dict_LeNet_WeightAct.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72396db9-ef9b-4f17-bfda-3e5c6e26184c",
   "metadata": {},
   "source": [
    "####  Convert to ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "864c180c-c4d7-496f-85f1-b11d9e74dbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/export/onnx/standard/manager.py:26: UserWarning: ONNX opset version set to 13, override with opset_version=\n",
      "  warnings.warn(f\"ONNX opset version set to {DEFAULT_OPSET}, override with {ka}=\")\n"
     ]
    }
   ],
   "source": [
    "# from brevitas.export import export_onnx_qcdq\n",
    "# import torch\n",
    "\n",
    "# # Weight-activation model\n",
    "# export_onnx_qcdq(model, torch.randn(1, 3, 32, 32), export_path='4b_weight_act_lenet.onnx');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a051b30-4cfe-4499-b25c-f875cbe83658",
   "metadata": {},
   "source": [
    "####  Convert to QONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "59a5072b-3776-4581-a4c5-63382e648998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to 4b_weight_act_lenet_qonnx.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inets/arish/FINN_arish/finn/deps/qonnx/src/qonnx/transformation/gemm_to_matmul.py:57: UserWarning: The GemmToMatMul transformation only offers explicit support for version 9 of the Gemm node, but the ONNX version of the supplied model is 14. Thus the transformation may fail or return incomplete results.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from brevitas.export import export_qonnx\n",
    "from qonnx.util.cleanup import cleanup as qonnx_cleanup\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from qonnx.core.datatype import DataType\n",
    "from finn.transformation.qonnx.convert_qonnx_to_finn import ConvertQONNXtoFINN\n",
    "\n",
    "ready_model_filename = '4b_weight_act_lenet_qonnx.onnx'\n",
    "input_shape = (1, 3, 32, 32)\n",
    "\n",
    "# create a QuantTensor instance to mark input as bipolar during export\n",
    "input_a = np.random.randint(0, 1, size=input_shape).astype(np.float32)\n",
    "input_a = 2 * input_a - 1\n",
    "scale = 1.0\n",
    "input_t = torch.from_numpy(input_a * scale)\n",
    "\n",
    "#Move to CPU before export\n",
    "model.cpu()\n",
    "\n",
    "# Export to ONNX\n",
    "export_qonnx(\n",
    "    model, export_path=ready_model_filename, input_t=input_t\n",
    ")\n",
    "\n",
    "# clean-up\n",
    "qonnx_cleanup(ready_model_filename, out_file=ready_model_filename)\n",
    "\n",
    "# ModelWrapper\n",
    "model = ModelWrapper(ready_model_filename)\n",
    "# Setting the input datatype explicitly because it doesn't get derived from the export function\n",
    "model.set_tensor_datatype(model.graph.input[0].name, DataType[\"BIPOLAR\"])\n",
    "model = model.transform(ConvertQONNXtoFINN())\n",
    "model.save(ready_model_filename)\n",
    "\n",
    "print(\"Model saved to %s\" % ready_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "44986f1e-c8c6-4481-9999-63c09cc14186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving '4b_weight_act_lenet_qonnx.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f632794ab00>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finn.util.visualization import showInNetron\n",
    "\n",
    "showInNetron(ready_model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70080407-8909-4b71-8dc5-f4660102620d",
   "metadata": {},
   "source": [
    "### Network surgery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "722e1160-1447-4598-a77b-35a342d80cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model to CPU before surgery\n",
    "# model = model.cpu()\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fffc0ebb-70a1-474b-ae4f-bbc2ed084dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "modified_model = deepcopy(model)\n",
    "\n",
    "# W_orig = modified_model.conv1.weight#.data.detach().numpy()\n",
    "# W_orig\n",
    "# W_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9678c2b8-0246-4bfe-a7d1-55219124ed62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from brevitas.nn import QuantIdentity\n",
    "\n",
    "\n",
    "class LeNetForExport(nn.Module):\n",
    "    def __init__(self, my_pretrained_model):\n",
    "        super(LeNetForExport, self).__init__()\n",
    "        self.pretrained = my_pretrained_model\n",
    "        self.qnt_output = QuantIdentity(\n",
    "            quant_type='binary', \n",
    "            scaling_impl_type='const',\n",
    "            bit_width=1, min_val=-1.0, max_val=1.0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # assume x contains bipolar {-1,1} elems\n",
    "        # shift from {-1,1} -> {0,1} since that is the\n",
    "        # input range for the trained network\n",
    "        x = (x + torch.tensor([1.0]).to(x.device)) / 2.0  \n",
    "        out_original = self.pretrained(x)\n",
    "        out_final = self.qnt_output(out_original)   # output as {-1,1}     \n",
    "        return out_final\n",
    "\n",
    "model_for_export = LeNetForExport(modified_model)\n",
    "model_for_export.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "563a4621-841c-4400-9506-65f87c8ce27d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[-0.0009,  0.0619, -0.0950, -0.0850, -0.0445],\n",
       "          [ 0.0310, -0.0023,  0.0916, -0.0102,  0.0306],\n",
       "          [-0.0349, -0.0227, -0.1103, -0.0765, -0.0476],\n",
       "          [ 0.0043,  0.0456,  0.0693, -0.0783, -0.0503],\n",
       "          [ 0.0419,  0.0959, -0.0238,  0.0864, -0.0186]],\n",
       "\n",
       "         [[ 0.0122,  0.1046, -0.1071, -0.0727, -0.0292],\n",
       "          [-0.0450,  0.0998, -0.0748, -0.0532, -0.0807],\n",
       "          [-0.1081, -0.0674,  0.0993,  0.0515,  0.0560],\n",
       "          [ 0.0061, -0.0592,  0.0195, -0.1078, -0.0834],\n",
       "          [-0.0595,  0.0729,  0.0677, -0.0512, -0.0042]],\n",
       "\n",
       "         [[ 0.0739,  0.1148,  0.0458,  0.0156,  0.0774],\n",
       "          [-0.0680,  0.0215, -0.0895, -0.0800, -0.0596],\n",
       "          [ 0.0522,  0.0464, -0.0684,  0.0349,  0.0634],\n",
       "          [-0.0146,  0.0044,  0.0268,  0.0716,  0.1109],\n",
       "          [-0.0890, -0.0423,  0.0454,  0.0957,  0.1005]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1019,  0.0230, -0.1004,  0.0106, -0.0722],\n",
       "          [-0.1076,  0.1026,  0.0878, -0.1152,  0.0216],\n",
       "          [-0.0195, -0.0190, -0.0529,  0.0444, -0.0684],\n",
       "          [ 0.0423,  0.0584,  0.0827,  0.0432, -0.1143],\n",
       "          [-0.0749,  0.0577,  0.0242, -0.0901, -0.0665]],\n",
       "\n",
       "         [[ 0.1086,  0.0778, -0.0503, -0.0291, -0.1100],\n",
       "          [-0.0021, -0.0870, -0.0891, -0.0064,  0.0173],\n",
       "          [-0.0473,  0.0685, -0.0703,  0.1048,  0.0791],\n",
       "          [-0.0974, -0.0287,  0.0052,  0.0168,  0.0274],\n",
       "          [ 0.0453,  0.0069, -0.0563,  0.0546, -0.1108]],\n",
       "\n",
       "         [[-0.0684, -0.0289, -0.0562, -0.0404, -0.0946],\n",
       "          [-0.0246,  0.0247, -0.0752, -0.0059,  0.0827],\n",
       "          [-0.0119,  0.0032, -0.0100,  0.0234,  0.0734],\n",
       "          [ 0.1094,  0.0733,  0.1096, -0.0084, -0.1037],\n",
       "          [-0.0547,  0.0786, -0.0007, -0.0574, -0.0885]]],\n",
       "\n",
       "\n",
       "        [[[-0.1081, -0.0975, -0.0234,  0.0633,  0.0624],\n",
       "          [-0.1114,  0.0720, -0.0904, -0.0244, -0.0468],\n",
       "          [-0.0222, -0.0227, -0.1036, -0.0997, -0.0181],\n",
       "          [ 0.0015, -0.0525,  0.0435, -0.1039, -0.0078],\n",
       "          [ 0.1015, -0.0471,  0.1043,  0.0418, -0.1042]],\n",
       "\n",
       "         [[ 0.0731, -0.0133, -0.0515,  0.0923, -0.0933],\n",
       "          [ 0.0124, -0.0242,  0.0825,  0.0322,  0.0555],\n",
       "          [ 0.0408, -0.0278, -0.0243, -0.0952,  0.0626],\n",
       "          [ 0.0917,  0.0790, -0.0815,  0.0051, -0.0814],\n",
       "          [-0.0636, -0.0673,  0.0395, -0.0688, -0.0025]],\n",
       "\n",
       "         [[ 0.0049,  0.0744, -0.0873, -0.0793, -0.0670],\n",
       "          [ 0.0808, -0.0415,  0.0974,  0.0418,  0.0146],\n",
       "          [-0.0009, -0.0228,  0.0145, -0.0264, -0.0008],\n",
       "          [ 0.0147, -0.0903, -0.0605,  0.0932, -0.0937],\n",
       "          [-0.0083,  0.1142,  0.0417,  0.0033, -0.1001]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0572, -0.0822, -0.0328, -0.0387, -0.0171],\n",
       "          [ 0.0013,  0.0952,  0.0144,  0.1034,  0.0706],\n",
       "          [-0.0730,  0.0518, -0.0816, -0.0489,  0.0340],\n",
       "          [ 0.0381,  0.0866, -0.0372,  0.0002,  0.0594],\n",
       "          [-0.1117,  0.0835, -0.0955,  0.0016, -0.0196]],\n",
       "\n",
       "         [[-0.0608,  0.0153,  0.0955, -0.0338, -0.0686],\n",
       "          [-0.0427, -0.1144,  0.0521, -0.0555, -0.0771],\n",
       "          [-0.0665,  0.0664,  0.0611,  0.0886,  0.0419],\n",
       "          [-0.0386, -0.0323,  0.0341,  0.0949,  0.0314],\n",
       "          [-0.0546, -0.0543, -0.1092,  0.0249, -0.0648]],\n",
       "\n",
       "         [[-0.1030,  0.1013, -0.0750, -0.0131,  0.0331],\n",
       "          [ 0.0037, -0.0777, -0.0933,  0.0920,  0.0188],\n",
       "          [ 0.0958, -0.0387,  0.0340, -0.0264, -0.0051],\n",
       "          [-0.0703,  0.0391,  0.0365, -0.0024, -0.0260],\n",
       "          [-0.0712,  0.0799, -0.0860,  0.0473, -0.0388]]],\n",
       "\n",
       "\n",
       "        [[[-0.0557,  0.0207, -0.0600,  0.0266,  0.0227],\n",
       "          [-0.0857,  0.0192,  0.0492,  0.0457, -0.0145],\n",
       "          [-0.0947, -0.0178,  0.0401, -0.0421,  0.0438],\n",
       "          [ 0.0769, -0.0603,  0.0011,  0.0477,  0.0091],\n",
       "          [ 0.0096,  0.0144, -0.0908,  0.0091,  0.0800]],\n",
       "\n",
       "         [[ 0.1041,  0.0679,  0.0155,  0.0539, -0.0562],\n",
       "          [-0.0957, -0.0993,  0.1152,  0.0733, -0.0798],\n",
       "          [ 0.0452,  0.0872,  0.1154,  0.1010,  0.0895],\n",
       "          [-0.0265, -0.0405,  0.0948,  0.0647, -0.0695],\n",
       "          [ 0.1038,  0.0558,  0.0629, -0.0724,  0.0331]],\n",
       "\n",
       "         [[-0.0405,  0.0902, -0.0208,  0.0450,  0.0205],\n",
       "          [ 0.0491, -0.0392,  0.0563, -0.0807,  0.0261],\n",
       "          [-0.0781, -0.1139, -0.0927,  0.0912,  0.0625],\n",
       "          [ 0.1083,  0.0925, -0.1031, -0.0788, -0.0187],\n",
       "          [-0.0750,  0.0802, -0.0873, -0.0563, -0.1116]]],\n",
       "\n",
       "\n",
       "        [[[-0.0656,  0.0950,  0.0945,  0.0827,  0.0892],\n",
       "          [ 0.1027, -0.0296,  0.0508,  0.1029,  0.0382],\n",
       "          [ 0.1154,  0.0599,  0.0718, -0.0404,  0.0554],\n",
       "          [ 0.0133, -0.0276, -0.0651, -0.0648, -0.0889],\n",
       "          [ 0.0775,  0.0821, -0.0131, -0.0668,  0.0892]],\n",
       "\n",
       "         [[ 0.0738,  0.0086, -0.0545,  0.1061,  0.0472],\n",
       "          [-0.0877,  0.1105,  0.0877, -0.0421,  0.0649],\n",
       "          [-0.0656, -0.0181,  0.0980,  0.0048, -0.0817],\n",
       "          [-0.0386, -0.0313, -0.0223,  0.0111,  0.1068],\n",
       "          [ 0.0062, -0.0713,  0.0059,  0.0554,  0.0573]],\n",
       "\n",
       "         [[-0.1055, -0.0207, -0.0858, -0.0493,  0.0416],\n",
       "          [-0.0820,  0.0429,  0.0980,  0.0076, -0.0770],\n",
       "          [-0.0414,  0.0252, -0.0880,  0.0574, -0.1048],\n",
       "          [-0.1110, -0.1122, -0.0234,  0.0776, -0.1093],\n",
       "          [ 0.0960, -0.0462,  0.0338,  0.0053, -0.1041]]]], requires_grad=True)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# W_orig = model_for_export.pretrained.conv1.weight#.data.detach().numpy()\n",
    "# W_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ddac8c8d-c78d-4709-ba40-3bd7b36a40b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def test_padded_bipolar(model, test_loader):    \n",
    "#     # ensure model is in eval mode\n",
    "#     model.eval() \n",
    "#     y_true = []\n",
    "#     y_pred = []\n",
    "   \n",
    "#     with torch.no_grad():\n",
    "#         for data in test_loader:\n",
    "#             inputs, target = data\n",
    "#             inputs, target = inputs.to(device), target.to(device)\n",
    "#             # pad inputs to 600 elements\n",
    "#             input_padded = torch.nn.functional.pad(inputs, (0,7,0,0))\n",
    "#             # convert inputs to {-1,+1}\n",
    "#             input_scaled = 2 * input_padded - 1\n",
    "#             # run the model\n",
    "#             output = model(input_scaled.float())\n",
    "#             y_pred.extend(list(output.flatten().cpu().numpy()))\n",
    "#             # make targets bipolar {-1,+1}\n",
    "#             expected = 2 * target.float() - 1\n",
    "#             expected = expected.cpu().numpy()\n",
    "#             y_true.extend(list(expected.flatten()))\n",
    "        \n",
    "#     return accuracy_score(y_true, y_pred)\n",
    "\n",
    "def test_padded_bipolar(model, test_loader):    \n",
    "    # ensure model is in eval mode\n",
    "    model.eval() \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "   \n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            output = model(images.float())\n",
    "            # run the output through sigmoid\n",
    "            # output = torch.sigmoid(output_orig)  \n",
    "            # compare against a threshold of 0.5 to generate 0/1\n",
    "            # pred = (output.detach().cpu().numpy() > 0.5) * 1\n",
    "            _, pred = torch.max(output.data, 1)\n",
    "            labels = labels.cpu().float()\n",
    "            y_true.extend(labels.tolist()) \n",
    "            y_pred.extend(pred.reshape(-1).tolist())\n",
    "            # y_pred.extend((pred == labels).sum().item())\n",
    "        \n",
    "    return accuracy_score(y_true, y_pred)\n",
    "\n",
    "test_padded_bipolar(model_for_export, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64dcc5d-1a5c-4a71-a45f-133716901e38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
