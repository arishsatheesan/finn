{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e4ce4eb3-1bef-4647-ba85-6b1abde93a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target device: cpu\n"
     ]
    }
   ],
   "source": [
    "# https://xilinx.github.io/brevitas/getting_started.html\n",
    "\n",
    "from torch.nn import Module\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import brevitas.nn as qnn\n",
    "from brevitas.quant import Int32Bias\n",
    "\n",
    "import os\n",
    "import onnx\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from brevitas.nn import QuantLinear, QuantReLU\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "weight_bit_width = 2\n",
    "act_bit_width = 2\n",
    "\n",
    "class QuantWeightActBiasLeNet(Module):\n",
    "    def __init__(self):\n",
    "        super(QuantWeightActBiasLeNet, self).__init__()\n",
    "        self.quant_inp = qnn.QuantIdentity(bit_width=4, return_quant_tensor=True)\n",
    "        self.conv1 = qnn.QuantConv2d(3, 6, 5, bias=True, weight_bit_width=weight_bit_width, bias_quant=Int32Bias)\n",
    "        self.relu1 = qnn.QuantReLU(bit_width=act_bit_width, return_quant_tensor=True)\n",
    "        self.conv2 = qnn.QuantConv2d(6, 16, 5, bias=True, weight_bit_width=weight_bit_width, bias_quant=Int32Bias)\n",
    "        self.relu2 = qnn.QuantReLU(bit_width=act_bit_width, return_quant_tensor=True)\n",
    "        self.fc1   = qnn.QuantLinear(16*10*10, 120, bias=True, weight_bit_width=weight_bit_width, bias_quant=Int32Bias)\n",
    "        # self.fc1   = qnn.QuantLinear(16*5*5, 120, bias=True, weight_bit_width=weight_bit_width, bias_quant=Int32Bias)\n",
    "        self.relu3 = qnn.QuantReLU(bit_width=act_bit_width, return_quant_tensor=True)\n",
    "        self.fc2   = qnn.QuantLinear(120, 84, bias=True, weight_bit_width=weight_bit_width, bias_quant=Int32Bias)\n",
    "        self.relu4 = qnn.QuantReLU(bit_width=act_bit_width, return_quant_tensor=True)\n",
    "        self.fc3   = qnn.QuantLinear(84, 10, bias=True, weight_bit_width=weight_bit_width, bias_quant=Int32Bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.quant_inp(x)\n",
    "        out = self.relu1(self.conv1(out))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = self.relu2(self.conv2(out))\n",
    "        # out = F.max_pool2d(out, 2)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.relu3(self.fc1(out))\n",
    "        out = self.relu4(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "model = QuantWeightActBiasLeNet()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Target device: \" + str(device))\n",
    "\n",
    "model.to(device); # The semicolon is for not printing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b286283b-6db0-4bbe-b44f-4cea3986a5cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9308b30f-dd2b-4618-b34a-f9f969b0e86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Saved the dataset as .npz files\n"
     ]
    }
   ],
   "source": [
    "# This is for 4-bit quantization. The quantization value can be changed in line 11. \n",
    "\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "\n",
    "quant_param = 4 # log2(quant_param) bits. #256 means no quanization, 1 means 1-bit quantization\n",
    "\n",
    "def quantize_image(image):\n",
    "    \"\"\"Quantize and binarize an image.\"\"\"\n",
    "    image = image.astype(np.float32)\n",
    "    image = np.floor(image / (256/quant_param))  # Example: reducing to 4-bit quantization\n",
    "    return image.astype(np.float32)\n",
    "\n",
    "def save_dataset_as_npz(data, labels, filename):\n",
    "    \"\"\"Save the dataset as a .npz file.\"\"\"\n",
    "    np.savez_compressed(filename, data=data, labels=labels)\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Quantize the images\n",
    "train_images = np.array([quantize_image(image.numpy().transpose(1, 2, 0) * 255) for image, _ in train_dataset])\n",
    "train_labels = np.array(train_dataset.targets)\n",
    "\n",
    "test_images = np.array([quantize_image(image.numpy().transpose(1, 2, 0) * 255) for image, _ in test_dataset])\n",
    "test_labels = np.array(test_dataset.targets)\n",
    "\n",
    "# Save the datasets\n",
    "os.makedirs('./quantized_data', exist_ok=True)\n",
    "save_dataset_as_npz(train_images, train_labels, './quantized_data/cifar10_train.npz')\n",
    "save_dataset_as_npz(test_images, test_labels, './quantized_data/cifar10_test.npz')\n",
    "\n",
    "print('Saved the dataset as .npz files')\n",
    "\n",
    "\n",
    "class CIFAR10QuantizedDataset(Dataset):\n",
    "    def __init__(self, npz_file):\n",
    "        data = np.load(npz_file)\n",
    "        self.images = data['data']\n",
    "        self.labels = data['labels']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx].astype(np.float32) / 255.0\n",
    "        label = self.labels[idx]\n",
    "        image = torch.tensor(image.transpose(2, 0, 1))  # HWC to CHW format\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return image, label\n",
    "\n",
    "# Load the quantized dataset\n",
    "train_dataset = CIFAR10QuantizedDataset('./quantized_data/cifar10_train.npz')\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "test_dataset = CIFAR10QuantizedDataset('./quantized_data/cifar10_test.npz')\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287be862-ebbe-461e-9e14-fb74bbfe957a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e3ca33dd-667a-4167-a8c3-3f6c09683d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss = 1.681720 test accuracy = 0.391600: 100%|█████████████████| 10/10 [01:50<00:00, 11.06s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train(model, train_loader, optimizer, criterion):\n",
    "    losses = []\n",
    "    # ensure model is in training mode\n",
    "    model.train()    \n",
    "    \n",
    "    for i, data in enumerate(train_loader, 0):        \n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()   \n",
    "                \n",
    "        # forward pass\n",
    "        output = model(images.float())\n",
    "        # loss = criterion(output, labels.unsqueeze(1))\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        # backward pass + run optimizer to update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # keep track of loss value\n",
    "        losses.append(loss.data.cpu().numpy()) \n",
    "    return losses\n",
    "\n",
    "\n",
    "def test(model, test_loader):    \n",
    "    # ensure model is in eval mode\n",
    "    model.eval() \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "   \n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            output = model(images.float())\n",
    "            # run the output through sigmoid\n",
    "            # output = torch.sigmoid(output_orig)  \n",
    "            # compare against a threshold of 0.5 to generate 0/1\n",
    "            # pred = (output.detach().cpu().numpy() > 0.5) * 1\n",
    "            _, pred = torch.max(output.data, 1)\n",
    "            labels = labels.cpu().float()\n",
    "            y_true.extend(labels.tolist()) \n",
    "            y_pred.extend(pred.reshape(-1).tolist())\n",
    "            # y_pred.extend((pred == labels).sum().item())\n",
    "        \n",
    "    return accuracy_score(y_true, y_pred)\n",
    "\n",
    "num_epochs = 10\n",
    "lr = 0.001 \n",
    "\n",
    "def display_loss_plot(losses, title=\"Training loss\", xlabel=\"Iterations\", ylabel=\"Loss\"):\n",
    "    x_axis = [i for i in range(len(losses))]\n",
    "    plt.plot(x_axis,losses)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()\n",
    "\n",
    "# loss criterion and optimizer\n",
    "# criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Setting seeds for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "running_loss = []\n",
    "running_test_acc = []\n",
    "t = trange(num_epochs, desc=\"Training loss\", leave=True)\n",
    "\n",
    "for epoch in t:\n",
    "        loss_epoch = train(model, train_loader, optimizer,criterion)\n",
    "        test_acc = test(model, test_loader)\n",
    "        t.set_description(\"Training loss = %f test accuracy = %f\" % (np.mean(loss_epoch), test_acc))\n",
    "        t.refresh() # to show immediately the update           \n",
    "        running_loss.append(loss_epoch)\n",
    "        running_test_acc.append(test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52b3bff-089a-4539-9841-800c7d78dc7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9e64f7c0-0cd0-44a3-bcb2-903569418ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy =  0.3916\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = test(model, test_loader)\n",
    "print('test accuracy = ', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7891899a-5e43-4b26-9c53-bb9ef2e1c499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the Brevitas model to disk\n",
    "# torch.save(model.state_dict(), \"state_dict_LeNet_WeightActBias.pth\")\n",
    "# # torch.save(model, \"LeNet_WeightActBias.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a82c1e7b-f663-43db-adb2-ee2abef60b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class QuantWeightActBiasLeNet(Module):\n",
    "#     def __init__(self):\n",
    "#         super(QuantWeightActBiasLeNet, self).__init__()\n",
    "#         self.quant_inp = qnn.QuantIdentity(bit_width=4, return_quant_tensor=True)\n",
    "#         self.conv1 = qnn.QuantConv2d(3, 6, 5, bias=True, weight_bit_width=weight_bit_width, bias_quant=Int32Bias)\n",
    "#         self.relu1 = qnn.QuantReLU(bit_width=act_bit_width, return_quant_tensor=True)\n",
    "#         self.conv2 = qnn.QuantConv2d(6, 16, 5, bias=True, weight_bit_width=weight_bit_width, bias_quant=Int32Bias)\n",
    "#         self.relu2 = qnn.QuantReLU(bit_width=act_bit_width, return_quant_tensor=True)\n",
    "#         self.fc1   = qnn.QuantLinear(16*5*5, 120, bias=True, weight_bit_width=weight_bit_width, bias_quant=Int32Bias)\n",
    "#         self.relu3 = qnn.QuantReLU(bit_width=act_bit_width, return_quant_tensor=True)\n",
    "#         self.fc2   = qnn.QuantLinear(120, 84, bias=True, weight_bit_width=weight_bit_width, bias_quant=Int32Bias)\n",
    "#         self.relu4 = qnn.QuantReLU(bit_width=act_bit_width, return_quant_tensor=True)\n",
    "#         self.fc3   = qnn.QuantLinear(84, 10, bias=True, weight_bit_width=weight_bit_width, bias_quant=Int32Bias)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out = self.quant_inp(x)\n",
    "#         out = self.relu1(self.conv1(out))\n",
    "#         out = F.max_pool2d(out, 2)\n",
    "#         out = self.relu2(self.conv2(out))\n",
    "#         out = F.max_pool2d(out, 2)\n",
    "#         out = out.reshape(out.shape[0], -1)\n",
    "#         out = self.relu3(self.fc1(out))\n",
    "#         out = self.relu4(self.fc2(out))\n",
    "#         out = self.fc3(out)\n",
    "#         return out\n",
    "\n",
    "# model = QuantWeightActBiasLeNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524785b7-58cc-42d5-8c98-6f23c2008fdf",
   "metadata": {},
   "source": [
    "####  Convert to ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b1141d4-e096-4863-a207-83e2dd0c7cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported graph: graph(%inp.1 : Float(1, 3, 32, 32, strides=[3072, 1024, 32, 1], requires_grad=0, device=cpu),\n",
      "      %conv1.bias : Float(6, strides=[1], requires_grad=1, device=cpu),\n",
      "      %conv2.bias : Float(16, strides=[1], requires_grad=1, device=cpu),\n",
      "      %fc1.bias : Float(120, strides=[1], requires_grad=1, device=cpu),\n",
      "      %fc2.bias : Float(84, strides=[1], requires_grad=1, device=cpu),\n",
      "      %fc3.bias : Float(10, strides=[1], requires_grad=1, device=cpu)):\n",
      "  %/quant_inp/act_quant/export_handler/Constant_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name=\"/quant_inp/act_quant/export_handler/Constant\"](), scope: __main__.QuantWeightActBiasLeNet::/brevitas.nn.quant_activation.QuantIdentity::quant_inp/brevitas.proxy.runtime_quant.ActQuantProxyFromInjector::act_quant/brevitas.export.onnx.qonnx.handler.BrevitasActQuantProxyHandler::export_handler # /home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/export/onnx/qonnx/handler.py:44:0\n",
      "  %/quant_inp/act_quant/export_handler/Constant_1_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={0.00199394}, onnx_name=\"/quant_inp/act_quant/export_handler/Constant_1\"](), scope: __main__.QuantWeightActBiasLeNet::/brevitas.nn.quant_activation.QuantIdentity::quant_inp/brevitas.proxy.runtime_quant.ActQuantProxyFromInjector::act_quant/brevitas.export.onnx.qonnx.handler.BrevitasActQuantProxyHandler::export_handler # /home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/export/onnx/qonnx/handler.py:47:0\n",
      "  %/quant_inp/act_quant/export_handler/Constant_2_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={0}, onnx_name=\"/quant_inp/act_quant/export_handler/Constant_2\"](), scope: __main__.QuantWeightActBiasLeNet::/brevitas.nn.quant_activation.QuantIdentity::quant_inp/brevitas.proxy.runtime_quant.ActQuantProxyFromInjector::act_quant/brevitas.export.onnx.qonnx.handler.BrevitasActQuantProxyHandler::export_handler # /home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/export/onnx/qonnx/handler.py:47:0\n",
      "  %/quant_inp/act_quant/export_handler/Quant_output_0 : Float(*, *, *, *, strides=[3072, 1024, 32, 1], requires_grad=0, device=cpu) = onnx.brevitas::Quant[narrow=0, rounding_mode=\"ROUND\", signed=1, onnx_name=\"/quant_inp/act_quant/export_handler/Quant\"](%inp.1, %/quant_inp/act_quant/export_handler/Constant_1_output_0, %/quant_inp/act_quant/export_handler/Constant_2_output_0, %/quant_inp/act_quant/export_handler/Constant_output_0), scope: __main__.QuantWeightActBiasLeNet::/brevitas.nn.quant_activation.QuantIdentity::quant_inp/brevitas.proxy.runtime_quant.ActQuantProxyFromInjector::act_quant/brevitas.export.onnx.qonnx.handler.BrevitasActQuantProxyHandler::export_handler # /home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/export/onnx/qonnx/handler.py:47:0\n",
      "  %/conv1/weight_quant/export_handler/Constant_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={2}, onnx_name=\"/conv1/weight_quant/export_handler/Constant\"](), scope: __main__.QuantWeightActBiasLeNet::/brevitas.nn.quant_conv.QuantConv2d::conv1/brevitas.proxy.parameter_quant.WeightQuantProxyFromInjector::weight_quant/brevitas.export.onnx.qonnx.handler.BrevitasWeightQuantProxyHandler::export_handler # /home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/export/onnx/qonnx/handler.py:44:0\n",
      "  %/conv1/weight_quant/export_handler/Constant_1_output_0 : Float(6, 3, 5, 5, strides=[75, 25, 5, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name=\"/conv1/weight_quant/export_handler/Constant_1\"](), scope: __main__.QuantWeightActBiasLeNet::/brevitas.nn.quant_conv.QuantConv2d::conv1/brevitas.proxy.parameter_quant.WeightQuantProxyFromInjector::weight_quant/brevitas.export.onnx.qonnx.handler.BrevitasWeightQuantProxyHandler::export_handler # /home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/export/onnx/qonnx/handler.py:47:0\n",
      "  %/conv1/weight_quant/export_handler/Constant_2_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={0.436712}, onnx_name=\"/conv1/weight_quant/export_handler/Constant_2\"](), scope: __main__.QuantWeightActBiasLeNet::/brevitas.nn.quant_conv.QuantConv2d::conv1/brevitas.proxy.parameter_quant.WeightQuantProxyFromInjector::weight_quant/brevitas.export.onnx.qonnx.handler.BrevitasWeightQuantProxyHandler::export_handler # /home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/export/onnx/qonnx/handler.py:47:0\n",
      "  %/conv1/weight_quant/export_handler/Quant_output_0 : Float(*, *, *, *, strides=[75, 25, 5, 1], requires_grad=0, device=cpu) = onnx.brevitas::Quant[narrow=1, rounding_mode=\"ROUND\", signed=1, onnx_name=\"/conv1/weight_quant/export_handler/Quant\"](%/conv1/weight_quant/export_handler/Constant_1_output_0, %/conv1/weight_quant/export_handler/Constant_2_output_0, %/quant_inp/act_quant/export_handler/Constant_2_output_0, %/conv1/weight_quant/export_handler/Constant_output_0), scope: __main__.QuantWeightActBiasLeNet::/brevitas.nn.quant_conv.QuantConv2d::conv1/brevitas.proxy.parameter_quant.WeightQuantProxyFromInjector::weight_quant/brevitas.export.onnx.qonnx.handler.BrevitasWeightQuantProxyHandler::export_handler # /home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/export/onnx/qonnx/handler.py:47:0\n",
      "  %onnx.brevitas::Quant_24 : Float(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={0.000870776}]()\n",
      "  %/conv1/bias_quant/export_handler/Constant_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={32}, onnx_name=\"/conv1/bias_quant/export_handler/Constant\"](), scope: __main__.QuantWeightActBiasLeNet::/brevitas.nn.quant_conv.QuantConv2d::conv1/brevitas.proxy.parameter_quant.BiasQuantProxyFromInjector::bias_quant/brevitas.export.onnx.qonnx.handler.BrevitasBiasQuantProxyHandler::export_handler # /home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/export/onnx/qonnx/handler.py:115:0\n",
      "  %/conv1/bias_quant/export_handler/Quant_output_0 : Float(*, strides=[1], requires_grad=0, device=cpu) = onnx.brevitas::Quant[narrow=0, rounding_mode=\"ROUND\", signed=1, onnx_name=\"/conv1/bias_quant/export_handler/Quant\"](%conv1.bias, %onnx.brevitas::Quant_24, %/quant_inp/act_quant/export_handler/Constant_2_output_0, %/conv1/bias_quant/export_handler/Constant_output_0), scope: __main__.QuantWeightActBiasLeNet::/brevitas.nn.quant_conv.QuantConv2d::conv1/brevitas.proxy.parameter_quant.BiasQuantProxyFromInjector::bias_quant/brevitas.export.onnx.qonnx.handler.BrevitasBiasQuantProxyHandler::export_handler # /home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/export/onnx/qonnx/handler.py:115:0\n",
      "  %/conv1/Conv_output_0 : Float(*, *, *, *, strides=[4704, 784, 28, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"/conv1/Conv\"](%/quant_inp/act_quant/export_handler/Quant_output_0, %/conv1/weight_quant/export_handler/Quant_output_0, %/conv1/bias_quant/export_handler/Quant_output_0), scope: __main__.QuantWeightActBiasLeNet::/brevitas.nn.quant_conv.QuantConv2d::conv1 # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:458:0\n",
      "  %/relu1/act_quant/activation_impl/Relu_output_0 : Float(*, *, *, *, strides=[4704, 784, 28, 1], requires_grad=0, device=cpu) = onnx::Relu[onnx_name=\"/relu1/act_quant/activation_impl/Relu\"](%/conv1/Conv_output_0), scope: __main__.QuantWeightActBiasLeNet::/brevitas.nn.quant_activation.QuantReLU::relu1/brevitas.proxy.runtime_quant.ActQuantProxyFromInjector::act_quant/torch.nn.modules.activation.ReLU::activation_impl # /usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:1453:0\n",
      "  %/relu1/act_quant/export_handler/Constant_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={0.0142352}, onnx_name=\"/relu1/act_quant/export_handler/Constant\"](), scope: __main__.QuantWeightActBiasLeNet::/brevitas.nn.quant_activation.QuantReLU::relu1/brevitas.proxy.runtime_quant.ActQuantProxyFromInjector::act_quant/brevitas.export.onnx.qonnx.handler.BrevitasActQuantProxyHandler::export_handler # /home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/export/onnx/qonnx/handler.py:47:0\n",
      "  %/relu1/act_quant/export_handler/Quant_output_0 : Float(*, *, *, *, strides=[4704, 784, 28, 1], requires_grad=0, device=cpu) = onnx.brevitas::Quant[narrow=0, rounding_mode=\"ROUND\", signed=0, onnx_name=\"/relu1/act_quant/export_handler/Quant\"](%/relu1/act_quant/activation_impl/Relu_output_0, %/relu1/act_quant/export_handler/Constant_output_0, %/quant_inp/act_quant/export_handler/Constant_2_output_0, %/conv1/weight_quant/export_handler/Constant_output_0), scope: __main__.QuantWeightActBiasLeNet::/brevitas.nn.quant_activation.QuantReLU::relu1/brevitas.proxy.runtime_quant.ActQuantProxyFromInjector::act_quant/brevitas.export.onnx.qonnx.handler.BrevitasActQuantProxyHandler::export_handler # /home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/export/onnx/qonnx/handler.py:47:0\n",
      "  %/MaxPool_output_0 : Float(*, *, *, *, strides=[1176, 196, 14, 1], requires_grad=0, device=cpu) = onnx::MaxPool[ceil_mode=0, kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2], onnx_name=\"/MaxPool\"](%/relu1/act_quant/export_handler/Quant_output_0), scope: __main__.QuantWeightActBiasLeNet:: # /usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:780:0\n",
      "  %/conv2/weight_quant/export_handler/Constant_output_0 : Float(16, 6, 5, 5, strides=[150, 25, 5, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name=\"/conv2/weight_quant/export_handler/Constant\"](), scope: __main__.QuantWeightActBiasLeNet::/brevitas.nn.quant_conv.QuantConv2d::conv2/brevitas.proxy.parameter_quant.WeightQuantProxyFromInjector::weight_quant/brevitas.export.onnx.qonnx.handler.BrevitasWeightQuantProxyHandler::export_handler # /home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/export/onnx/qonnx/handler.py:47:0\n",
      "  %/conv2/weight_quant/export_handler/Constant_1_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={0.624667}, onnx_name=\"/conv2/weight_quant/export_handler/Constant_1\"](), scope: __main__.QuantWeightActBiasLeNet::/brevitas.nn.quant_conv.QuantConv2d::conv2/brevitas.proxy.parameter_quant.WeightQuantProxyFromInjector::weight_quant/brevitas.export.onnx.qonnx.handler.BrevitasWeightQuantProxyHandler::export_handler # /home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/export/onnx/qonnx/handler.py:47:0\n",
      "  %/conv2/weight_quant/export_handler/Quant_output_0 : Float(*, *, *, *, strides=[150, 25, 5, 1], requires_grad=0, device=cpu) = onnx.brevitas::Quant[narrow=1, rounding_mode=\"ROUND\", signed=1, onnx_name=\"/conv2/weight_quant/export_handler/Quant\"](%/conv2/weight_quant/export_handler/Constant_output_0, %/conv2/weight_quant/export_handler/Constant_1_output_0, %/quant_inp/act_quant/export_handler/Constant_2_output_0, %/conv1/weight_quant/export_handler/Constant_output_0), scope: __main__.QuantWeightActBiasLeNet::/brevitas.nn.quant_conv.QuantConv2d::conv2/brevitas.proxy.parameter_quant.WeightQuantProxyFromInjector::weight_quant/brevitas.export.onnx.qonnx.handler.BrevitasWeightQuantProxyHandler::export_handler # /home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/export/onnx/qonnx/handler.py:47:0\n",
      "  %onnx.brevitas::Quant_35 : Float(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={0.00889225}]()\n",
      "  %/conv2/bias_quant/export_handler/Quant_output_0 : Float(*, strides=[1], requires_grad=0, device=cpu) = onnx.brevitas::Quant[narrow=0, rounding_mode=\"ROUND\", signed=1, onnx_name=\"/conv2/bias_quant/export_handler/Quant\"](%conv2.bias, %onnx.brevitas::Quant_35, %/quant_inp/act_quant/export_handler/Constant_2_output_0, %/conv1/bias_quant/export_handler/Constant_output_0), scope: __main__.QuantWeightActBiasLeNet::/brevitas.nn.quant_conv.QuantConv2d::conv2/brevitas.proxy.parameter_quant.BiasQuantProxyFromInjector::bias_quant/brevitas.export.onnx.qonnx.handler.BrevitasBiasQuantProxyHandler::export_handler # /home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/export/onnx/qonnx/handler.py:115:0\n",
      "  %/conv2/Conv_output_0 : Float(*, *, *, *, strides=[1600, 100, 10, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"/conv2/Conv\"](%/MaxPool_output_0, %/conv2/weight_quant/export_handler/Quant_output_0, %/conv2/bias_quant/export_handler/Quant_output_0), scope: __main__.QuantWeightActBiasLeNet::/brevitas.nn.quant_conv.QuantConv2d::conv2 # /usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:458:0\n",
      "  %/relu2/act_quant/activation_impl/Relu_output_0 : Float(*, *, *, *, strides=[1600, 100, 10, 1], requires_grad=0, device=cpu) = onnx::Relu[onnx_name=\"/relu2/act_quant/activation_impl/Relu\"](%/conv2/Conv_output_0), scope: __main__.QuantWeightActBiasLeNet::/brevitas.nn.quant_activation.QuantReLU::relu2/brevitas.proxy.runtime_quant.ActQuantProxyFromInjector::act_quant/torch.nn.modules.activation.ReLU::activation_impl # /usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:1453:0\n",
      "  %/relu2/act_quant/export_handler/Constant_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={0.0452308}, onnx_name=\"/relu2/act_quant/export_handler/Constant\"](), scope: __main__.QuantWeightActBiasLeNet::/brevitas.nn.quant_activation.QuantReLU::relu2/brevitas.proxy.runtime_quant.ActQuantProxyFromInjector::act_quant/brevitas.export.onnx.qonnx.handler.BrevitasActQuantProxyHandler::export_handler # /home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/export/onnx/qonnx/handler.py:47:0\n",
      "  %/relu2/act_quant/export_handler/Quant_output_0 : Float(*, *, *, *, strides=[1600, 100, 10, 1], requires_grad=0, device=cpu) = onnx.brevitas::Quant[narrow=0, rounding_mode=\"ROUND\", signed=0, onnx_name=\"/relu2/act_quant/export_handler/Quant\"](%/relu2/act_quant/activation_impl/Relu_output_0, %/relu2/act_quant/export_handler/Constant_output_0, %/quant_inp/act_quant/export_handler/Constant_2_output_0, %/conv1/weight_quant/export_handler/Constant_output_0), scope: __main__.QuantWeightActBiasLeNet::/brevitas.nn.quant_activation.QuantReLU::relu2/brevitas.proxy.runtime_quant.ActQuantProxyFromInjector::act_quant/brevitas.export.onnx.qonnx.handler.BrevitasActQuantProxyHandler::export_handler # /home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/export/onnx/qonnx/handler.py:47:0\n",
      "  %/MaxPool_1_output_0 : Float(*, *, *, *, strides=[400, 25, 5, 1], requires_grad=0, device=cpu) = onnx::MaxPool[ceil_mode=0, kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2], onnx_name=\"/MaxPool_1\"](%/relu2/act_quant/export_handler/Quant_output_0), scope: __main__.QuantWeightActBiasLeNet:: # /usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:780:0\n",
      "  %/Shape_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name=\"/Shape\"](%/MaxPool_1_output_0), scope: __main__.QuantWeightActBiasLeNet:: # /home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/quant_tensor/__init__.py:255:0\n",
      "  %/Constant_output_0 : Long(device=cpu) = onnx::Constant[value={0}, onnx_name=\"/Constant\"](), scope: __main__.QuantWeightActBiasLeNet:: # /home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/quant_tensor/__init__.py:255:0\n",
      "  %/Gather_output_0 : Long(device=cpu) = onnx::Gather[axis=0, onnx_name=\"/Gather\"](%/Shape_output_0, %/Constant_output_0), scope: __main__.QuantWeightActBiasLeNet:: # /home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/quant_tensor/__init__.py:255:0\n",
      "  %onnx::Unsqueeze_46 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}]()\n",
      "  %/Unsqueeze_output_0 : Long(1, strides=[1], device=cpu) = onnx::Unsqueeze[onnx_name=\"/Unsqueeze\"](%/Gather_output_0, %onnx::Unsqueeze_46), scope: __main__.QuantWeightActBiasLeNet::\n",
      "  %/Constant_1_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name=\"/Constant_1\"](), scope: __main__.QuantWeightActBiasLeNet::\n",
      "  %/Concat_output_0 : Long(2, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name=\"/Concat\"](%/Unsqueeze_output_0, %/Constant_1_output_0), scope: __main__.QuantWeightActBiasLeNet:: # /home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/quant_tensor/__init__.py:227:0\n",
      "  %/Reshape_output_0 : Float(*, *, strides=[400, 1], requires_grad=0, device=cpu) = onnx::Reshape[allowzero=0, onnx_name=\"/Reshape\"](%/MaxPool_1_output_0, %/Concat_output_0), scope: __main__.QuantWeightActBiasLeNet:: # /home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/quant_tensor/__init__.py:227:0\n",
      "  %/fc1/weight_quant/export_handler/Constant_output_0 : Float(120, 400, strides=[400, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name=\"/fc1/weight_quant/export_handler/Constant\"](), scope: __main__.QuantWeightActBiasLeNet::/brevitas.nn.quant_linear.QuantLinear::fc1/brevitas.proxy.parameter_quant.WeightQuantProxyFromInjector::weight_quant/brevitas.export.onnx.qonnx.handler.BrevitasWeightQuantProxyHandler::export_handler # /home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/export/onnx/qonnx/handler.py:47:0\n",
      "  %/fc1/weight_quant/export_handler/Constant_1_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={0.547701}, onnx_name=\"/fc1/weight_quant/export_handler/Constant_1\"](), scope: __main__.QuantWeightActBiasLeNet::/brevitas.nn.quant_linear.QuantLinear::fc1/brevitas.proxy.parameter_quant.WeightQuantProxyFromInjector::weight_quant/brevitas.export.onnx.qonnx.handler.BrevitasWeightQuantProxyHandler::export_handler # /home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/export/onnx/qonnx/handler.py:47:0\n",
      "  %/fc1/weight_quant/export_handler/Quant_output_0 : Float(*, *, strides=[400, 1], requires_grad=0, device=cpu) = onnx.brevitas::Quant[narrow=1, rounding_mode=\"ROUND\", signed=1, onnx_name=\"/fc1/weight_quant/export_handler/Quant\"](%/fc1/weight_quant/export_handler/Constant_output_0, %/fc1/weight_quant/export_handler/Constant_1_output_0, %/quant_inp/act_quant/export_handler/Constant_2_output_0, %/conv1/weight_quant/export_handler/Constant_output_0), scope: __main__.QuantWeightActBiasLeNet::/brevitas.nn.quant_linear.QuantLinear::fc1/brevitas.proxy.parameter_quant.WeightQuantProxyFromInjector::weight_quant/brevitas.export.onnx.qonnx.handler.BrevitasWeightQuantProxyHandler::export_handler # /home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/export/onnx/qonnx/handler.py:47:0\n",
      "  %onnx.brevitas::Quant_55 : Float(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={0.0247729}]()\n",
      "  %/fc1/bias_quant/export_handler/Quant_output_0 : Float(*, strides=[1], requires_grad=0, device=cpu) = onnx.brevitas::Quant[narrow=0, rounding_mode=\"ROUND\", signed=1, onnx_name=\"/fc1/bias_quant/export_handler/Quant\"](%fc1.bias, %onnx.brevitas::Quant_55, %/quant_inp/act_quant/export_handler/Constant_2_output_0, %/conv1/bias_quant/export_handler/Constant_output_0), scope: __main__.QuantWeightActBiasLeNet::/brevitas.nn.quant_linear.QuantLinear::fc1/brevitas.proxy.parameter_quant.BiasQuantProxyFromInjector::bias_quant/brevitas.export.onnx.qonnx.handler.BrevitasBiasQuantProxyHandler::export_handler # /home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/export/onnx/qonnx/handler.py:115:0\n",
      "  %/fc1/Gemm_output_0 : Float(*, *, strides=[120, 1], requires_grad=0, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc1/Gemm\"](%/Reshape_output_0, %/fc1/weight_quant/export_handler/Quant_output_0, %/fc1/bias_quant/export_handler/Quant_output_0), scope: __main__.QuantWeightActBiasLeNet::/brevitas.nn.quant_linear.QuantLinear::fc1 # /home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/nn/quant_linear.py:69:0\n",
      "  %/relu3/act_quant/activation_impl/Relu_output_0 : Float(*, *, strides=[120, 1], requires_grad=0, device=cpu) = onnx::Relu[onnx_name=\"/relu3/act_quant/activation_impl/Relu\"](%/fc1/Gemm_output_0), scope: __main__.QuantWeightActBiasLeNet::/brevitas.nn.quant_activation.QuantReLU::relu3/brevitas.proxy.runtime_quant.ActQuantProxyFromInjector::act_quant/torch.nn.modules.activation.ReLU::activation_impl # /usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:1453:0\n",
      "  %/relu3/act_quant/export_handler/Constant_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={0.171769}, onnx_name=\"/relu3/act_quant/export_handler/Constant\"](), scope: __main__.QuantWeightActBiasLeNet::/brevitas.nn.quant_activation.QuantReLU::relu3/brevitas.proxy.runtime_quant.ActQuantProxyFromInjector::act_quant/brevitas.export.onnx.qonnx.handler.BrevitasActQuantProxyHandler::export_handler # /home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/export/onnx/qonnx/handler.py:47:0\n",
      "  %/relu3/act_quant/export_handler/Quant_output_0 : Float(*, *, strides=[120, 1], requires_grad=0, device=cpu) = onnx.brevitas::Quant[narrow=0, rounding_mode=\"ROUND\", signed=0, onnx_name=\"/relu3/act_quant/export_handler/Quant\"](%/relu3/act_quant/activation_impl/Relu_output_0, %/relu3/act_quant/export_handler/Constant_output_0, %/quant_inp/act_quant/export_handler/Constant_2_output_0, %/conv1/weight_quant/export_handler/Constant_output_0), scope: __main__.QuantWeightActBiasLeNet::/brevitas.nn.quant_activation.QuantReLU::relu3/brevitas.proxy.runtime_quant.ActQuantProxyFromInjector::act_quant/brevitas.export.onnx.qonnx.handler.BrevitasActQuantProxyHandler::export_handler # /home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/export/onnx/qonnx/handler.py:47:0\n",
      "  %/fc2/weight_quant/export_handler/Constant_output_0 : Float(84, 120, strides=[120, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name=\"/fc2/weight_quant/export_handler/Constant\"](), scope: __main__.QuantWeightActBiasLeNet::/brevitas.nn.quant_linear.QuantLinear::fc2/brevitas.proxy.parameter_quant.WeightQuantProxyFromInjector::weight_quant/brevitas.export.onnx.qonnx.handler.BrevitasWeightQuantProxyHandler::export_handler # /home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/export/onnx/qonnx/handler.py:47:0\n",
      "  %/fc2/weight_quant/export_handler/Constant_1_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={0.429792}, onnx_name=\"/fc2/weight_quant/export_handler/Constant_1\"](), scope: __main__.QuantWeightActBiasLeNet::/brevitas.nn.quant_linear.QuantLinear::fc2/brevitas.proxy.parameter_quant.WeightQuantProxyFromInjector::weight_quant/brevitas.export.onnx.qonnx.handler.BrevitasWeightQuantProxyHandler::export_handler # /home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/export/onnx/qonnx/handler.py:47:0\n",
      "  %/fc2/weight_quant/export_handler/Quant_output_0 : Float(*, *, strides=[120, 1], requires_grad=0, device=cpu) = onnx.brevitas::Quant[narrow=1, rounding_mode=\"ROUND\", signed=1, onnx_name=\"/fc2/weight_quant/export_handler/Quant\"](%/fc2/weight_quant/export_handler/Constant_output_0, %/fc2/weight_quant/export_handler/Constant_1_output_0, %/quant_inp/act_quant/export_handler/Constant_2_output_0, %/conv1/weight_quant/export_handler/Constant_output_0), scope: __main__.QuantWeightActBiasLeNet::/brevitas.nn.quant_linear.QuantLinear::fc2/brevitas.proxy.parameter_quant.WeightQuantProxyFromInjector::weight_quant/brevitas.export.onnx.qonnx.handler.BrevitasWeightQuantProxyHandler::export_handler # /home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/export/onnx/qonnx/handler.py:47:0\n",
      "  %onnx.brevitas::Quant_64 : Float(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={0.073825}]()\n",
      "  %/fc2/bias_quant/export_handler/Quant_output_0 : Float(*, strides=[1], requires_grad=0, device=cpu) = onnx.brevitas::Quant[narrow=0, rounding_mode=\"ROUND\", signed=1, onnx_name=\"/fc2/bias_quant/export_handler/Quant\"](%fc2.bias, %onnx.brevitas::Quant_64, %/quant_inp/act_quant/export_handler/Constant_2_output_0, %/conv1/bias_quant/export_handler/Constant_output_0), scope: __main__.QuantWeightActBiasLeNet::/brevitas.nn.quant_linear.QuantLinear::fc2/brevitas.proxy.parameter_quant.BiasQuantProxyFromInjector::bias_quant/brevitas.export.onnx.qonnx.handler.BrevitasBiasQuantProxyHandler::export_handler # /home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/export/onnx/qonnx/handler.py:115:0\n",
      "  %/fc2/Gemm_output_0 : Float(*, *, strides=[84, 1], requires_grad=0, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc2/Gemm\"](%/relu3/act_quant/export_handler/Quant_output_0, %/fc2/weight_quant/export_handler/Quant_output_0, %/fc2/bias_quant/export_handler/Quant_output_0), scope: __main__.QuantWeightActBiasLeNet::/brevitas.nn.quant_linear.QuantLinear::fc2 # /home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/nn/quant_linear.py:69:0\n",
      "  %/relu4/act_quant/activation_impl/Relu_output_0 : Float(*, *, strides=[84, 1], requires_grad=0, device=cpu) = onnx::Relu[onnx_name=\"/relu4/act_quant/activation_impl/Relu\"](%/fc2/Gemm_output_0), scope: __main__.QuantWeightActBiasLeNet::/brevitas.nn.quant_activation.QuantReLU::relu4/brevitas.proxy.runtime_quant.ActQuantProxyFromInjector::act_quant/torch.nn.modules.activation.ReLU::activation_impl # /usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:1453:0\n",
      "  %/relu4/act_quant/export_handler/Constant_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={0.31098}, onnx_name=\"/relu4/act_quant/export_handler/Constant\"](), scope: __main__.QuantWeightActBiasLeNet::/brevitas.nn.quant_activation.QuantReLU::relu4/brevitas.proxy.runtime_quant.ActQuantProxyFromInjector::act_quant/brevitas.export.onnx.qonnx.handler.BrevitasActQuantProxyHandler::export_handler # /home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/export/onnx/qonnx/handler.py:47:0\n",
      "  %/relu4/act_quant/export_handler/Quant_output_0 : Float(*, *, strides=[84, 1], requires_grad=0, device=cpu) = onnx.brevitas::Quant[narrow=0, rounding_mode=\"ROUND\", signed=0, onnx_name=\"/relu4/act_quant/export_handler/Quant\"](%/relu4/act_quant/activation_impl/Relu_output_0, %/relu4/act_quant/export_handler/Constant_output_0, %/quant_inp/act_quant/export_handler/Constant_2_output_0, %/conv1/weight_quant/export_handler/Constant_output_0), scope: __main__.QuantWeightActBiasLeNet::/brevitas.nn.quant_activation.QuantReLU::relu4/brevitas.proxy.runtime_quant.ActQuantProxyFromInjector::act_quant/brevitas.export.onnx.qonnx.handler.BrevitasActQuantProxyHandler::export_handler # /home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/export/onnx/qonnx/handler.py:47:0\n",
      "  %/fc3/weight_quant/export_handler/Constant_output_0 : Float(10, 84, strides=[84, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name=\"/fc3/weight_quant/export_handler/Constant\"](), scope: __main__.QuantWeightActBiasLeNet::/brevitas.nn.quant_linear.QuantLinear::fc3/brevitas.proxy.parameter_quant.WeightQuantProxyFromInjector::weight_quant/brevitas.export.onnx.qonnx.handler.BrevitasWeightQuantProxyHandler::export_handler # /home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/export/onnx/qonnx/handler.py:47:0\n",
      "  %/fc3/weight_quant/export_handler/Constant_1_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={0.324572}, onnx_name=\"/fc3/weight_quant/export_handler/Constant_1\"](), scope: __main__.QuantWeightActBiasLeNet::/brevitas.nn.quant_linear.QuantLinear::fc3/brevitas.proxy.parameter_quant.WeightQuantProxyFromInjector::weight_quant/brevitas.export.onnx.qonnx.handler.BrevitasWeightQuantProxyHandler::export_handler # /home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/export/onnx/qonnx/handler.py:47:0\n",
      "  %/fc3/weight_quant/export_handler/Quant_output_0 : Float(*, *, strides=[84, 1], requires_grad=0, device=cpu) = onnx.brevitas::Quant[narrow=1, rounding_mode=\"ROUND\", signed=1, onnx_name=\"/fc3/weight_quant/export_handler/Quant\"](%/fc3/weight_quant/export_handler/Constant_output_0, %/fc3/weight_quant/export_handler/Constant_1_output_0, %/quant_inp/act_quant/export_handler/Constant_2_output_0, %/conv1/weight_quant/export_handler/Constant_output_0), scope: __main__.QuantWeightActBiasLeNet::/brevitas.nn.quant_linear.QuantLinear::fc3/brevitas.proxy.parameter_quant.WeightQuantProxyFromInjector::weight_quant/brevitas.export.onnx.qonnx.handler.BrevitasWeightQuantProxyHandler::export_handler # /home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/export/onnx/qonnx/handler.py:47:0\n",
      "  %onnx.brevitas::Quant_73 : Float(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={0.100935}]()\n",
      "  %/fc3/bias_quant/export_handler/Quant_output_0 : Float(*, strides=[1], requires_grad=0, device=cpu) = onnx.brevitas::Quant[narrow=0, rounding_mode=\"ROUND\", signed=1, onnx_name=\"/fc3/bias_quant/export_handler/Quant\"](%fc3.bias, %onnx.brevitas::Quant_73, %/quant_inp/act_quant/export_handler/Constant_2_output_0, %/conv1/bias_quant/export_handler/Constant_output_0), scope: __main__.QuantWeightActBiasLeNet::/brevitas.nn.quant_linear.QuantLinear::fc3/brevitas.proxy.parameter_quant.BiasQuantProxyFromInjector::bias_quant/brevitas.export.onnx.qonnx.handler.BrevitasBiasQuantProxyHandler::export_handler # /home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/export/onnx/qonnx/handler.py:115:0\n",
      "  %75 : Float(*, *, strides=[10, 1], requires_grad=0, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc3/Gemm\"](%/relu4/act_quant/export_handler/Quant_output_0, %/fc3/weight_quant/export_handler/Quant_output_0, %/fc3/bias_quant/export_handler/Quant_output_0), scope: __main__.QuantWeightActBiasLeNet::/brevitas.nn.quant_linear.QuantLinear::fc3 # /home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/nn/quant_linear.py:69:0\n",
      "  return (%75)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inets/arish/FINN_arish/finn/deps/qonnx/src/qonnx/transformation/extract_conv_bias.py:52: UserWarning: Could not extract bias from node input: \"Quant_0_out0\"\n",
      "input: \"Quant_1_out0\"\n",
      "input: \"Quant_2_out0\"\n",
      "output: \"Conv_0_out0\"\n",
      "name: \"Conv_0\"\n",
      "op_type: \"Conv\"\n",
      "attribute {\n",
      "  name: \"dilations\"\n",
      "  ints: 1\n",
      "  ints: 1\n",
      "  type: INTS\n",
      "}\n",
      "attribute {\n",
      "  name: \"group\"\n",
      "  i: 1\n",
      "  type: INT\n",
      "}\n",
      "attribute {\n",
      "  name: \"kernel_shape\"\n",
      "  ints: 5\n",
      "  ints: 5\n",
      "  type: INTS\n",
      "}\n",
      "attribute {\n",
      "  name: \"pads\"\n",
      "  ints: 0\n",
      "  ints: 0\n",
      "  ints: 0\n",
      "  ints: 0\n",
      "  type: INTS\n",
      "}\n",
      "attribute {\n",
      "  name: \"strides\"\n",
      "  ints: 1\n",
      "  ints: 1\n",
      "  type: INTS\n",
      "}\n",
      "doc_string: \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py(458): _conv_forward\\n/home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/nn/quant_conv.py(202): inner_forward_impl\\n/home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/nn/quant_layer.py(318): forward_impl\\n/home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/nn/quant_conv.py(198): forward\\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py(1176): _slow_forward\\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py(1192): _call_impl\\n/tmp/ipykernel_147/1621294683.py(39): forward\\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py(1176): _slow_forward\\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py(1192): _call_impl\\n/usr/local/lib/python3.10/dist-packages/torch/jit/_trace.py(111): wrapper\\n/usr/local/lib/python3.10/dist-packages/torch/jit/_trace.py(104): forward\\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py(1192): _call_impl\\n/usr/local/lib/python3.10/dist-packages/torch/jit/_trace.py(1183): _get_trace_graph\\n/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py(890): _trace_and_get_graph_from_model\\n/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py(967): _create_jit_graph\\n/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py(1108): _model_to_graph\\n/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py(1501): _export\\n/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py(512): export\\n/home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/export/onnx/manager.py(103): export_onnx\\n/home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/export/onnx/manager.py(163): export\\n/home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/export/__init__.py(21): export_qonnx\\n/tmp/ipykernel_147/141494036.py(26): <module>\\n/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py(3574): run_code\\n/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py(3485): run_ast_nodes\\n/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py(3307): run_cell_async\\n/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py(128): _pseudo_sync_runner\\n/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py(3106): _run_cell\\n/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py(3076): run_cell\\n/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py(549): run_cell\\n/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py(416): do_execute\\n/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py(748): execute_request\\n/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py(361): execute_request\\n/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py(413): dispatch_shell\\n/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py(531): process_one\\n/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py(545): dispatch_queue\\n/usr/lib/python3.10/asyncio/events.py(80): _run\\n/usr/lib/python3.10/asyncio/base_events.py(1871): _run_once\\n/usr/lib/python3.10/asyncio/base_events.py(597): run_forever\\n/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py(205): start\\n/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py(729): start\\n/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py(1073): launch_instance\\n/usr/local/lib/python3.10/dist-packages/ipykernel_launcher.py(13): <module>\\n/usr/lib/python3.10/runpy.py(75): _run_code\\n/usr/lib/python3.10/runpy.py(191): _run_module_as_main\\n\"\n",
      "\n",
      "  warnings.warn(f\"Could not extract bias from node {n}\")\n",
      "/home/inets/arish/FINN_arish/finn/deps/qonnx/src/qonnx/transformation/extract_conv_bias.py:52: UserWarning: Could not extract bias from node input: \"MaxPool_0_out0\"\n",
      "input: \"Quant_3_out0\"\n",
      "input: \"Quant_4_out0\"\n",
      "output: \"Conv_1_out0\"\n",
      "name: \"Conv_1\"\n",
      "op_type: \"Conv\"\n",
      "attribute {\n",
      "  name: \"dilations\"\n",
      "  ints: 1\n",
      "  ints: 1\n",
      "  type: INTS\n",
      "}\n",
      "attribute {\n",
      "  name: \"group\"\n",
      "  i: 1\n",
      "  type: INT\n",
      "}\n",
      "attribute {\n",
      "  name: \"kernel_shape\"\n",
      "  ints: 5\n",
      "  ints: 5\n",
      "  type: INTS\n",
      "}\n",
      "attribute {\n",
      "  name: \"pads\"\n",
      "  ints: 0\n",
      "  ints: 0\n",
      "  ints: 0\n",
      "  ints: 0\n",
      "  type: INTS\n",
      "}\n",
      "attribute {\n",
      "  name: \"strides\"\n",
      "  ints: 1\n",
      "  ints: 1\n",
      "  type: INTS\n",
      "}\n",
      "doc_string: \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py(458): _conv_forward\\n/home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/nn/quant_conv.py(202): inner_forward_impl\\n/home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/nn/quant_layer.py(318): forward_impl\\n/home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/nn/quant_conv.py(198): forward\\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py(1176): _slow_forward\\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py(1192): _call_impl\\n/tmp/ipykernel_147/1621294683.py(39): forward\\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py(1176): _slow_forward\\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py(1192): _call_impl\\n/usr/local/lib/python3.10/dist-packages/torch/jit/_trace.py(111): wrapper\\n/usr/local/lib/python3.10/dist-packages/torch/jit/_trace.py(104): forward\\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py(1192): _call_impl\\n/usr/local/lib/python3.10/dist-packages/torch/jit/_trace.py(1183): _get_trace_graph\\n/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py(890): _trace_and_get_graph_from_model\\n/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py(967): _create_jit_graph\\n/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py(1108): _model_to_graph\\n/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py(1501): _export\\n/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py(512): export\\n/home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/export/onnx/manager.py(103): export_onnx\\n/home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/export/onnx/manager.py(163): export\\n/home/inets/arish/FINN_arish/finn/deps/brevitas/src/brevitas/export/__init__.py(21): export_qonnx\\n/tmp/ipykernel_147/141494036.py(26): <module>\\n/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py(3574): run_code\\n/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py(3485): run_ast_nodes\\n/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py(3307): run_cell_async\\n/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py(128): _pseudo_sync_runner\\n/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py(3106): _run_cell\\n/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py(3076): run_cell\\n/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py(549): run_cell\\n/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py(416): do_execute\\n/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py(748): execute_request\\n/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py(361): execute_request\\n/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py(413): dispatch_shell\\n/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py(531): process_one\\n/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py(545): dispatch_queue\\n/usr/lib/python3.10/asyncio/events.py(80): _run\\n/usr/lib/python3.10/asyncio/base_events.py(1871): _run_once\\n/usr/lib/python3.10/asyncio/base_events.py(597): run_forever\\n/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py(205): start\\n/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py(729): start\\n/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py(1073): launch_instance\\n/usr/local/lib/python3.10/dist-packages/ipykernel_launcher.py(13): <module>\\n/usr/lib/python3.10/runpy.py(75): _run_code\\n/usr/lib/python3.10/runpy.py(191): _run_module_as_main\\n\"\n",
      "\n",
      "  warnings.warn(f\"Could not extract bias from node {n}\")\n",
      "/home/inets/arish/FINN_arish/finn/deps/qonnx/src/qonnx/transformation/gemm_to_matmul.py:57: UserWarning: The GemmToMatMul transformation only offers explicit support for version 9 of the Gemm node, but the ONNX version of the supplied model is 14. Thus the transformation may fail or return incomplete results.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# from brevitas.export import export_onnx_qcdq\n",
    "# import torch\n",
    "\n",
    "# # Weight-activation-bias model\n",
    "# export_onnx_qcdq(quant_weight_act_bias_lenet, torch.randn(1, 3, 32, 32), export_path='./4b_weight_act_bias_lenet.onnx')\n",
    "import torch\n",
    "import onnx\n",
    "from finn.util.test import get_test_model_trained\n",
    "from brevitas.export import export_qonnx\n",
    "from qonnx.util.cleanup import cleanup as qonnx_cleanup\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from finn.transformation.qonnx.convert_qonnx_to_finn import ConvertQONNXtoFINN\n",
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "from qonnx.transformation.fold_constants import FoldConstants\n",
    "from qonnx.transformation.general import GiveReadableTensorNames, GiveUniqueNodeNames, RemoveStaticGraphInputs\n",
    "\n",
    "# # cnv = get_test_model_trained(\"LeNet_WeightActBias.pth\", 1, 1)\n",
    "# model_path = \"state_dict_LeNet_WeightActBias.pth\"\n",
    "# # model_path = \"LeNet_WeightActBias.pth\"\n",
    "# state_dict = torch.load(model_path)  # This returns an OrderedDict\n",
    "# model.load_state_dict(state_dict)  # Load the state dict into the model\n",
    "# # model = torch.load(model_path)\n",
    "# model.eval()\n",
    "\n",
    "export_onnx_path = \"LeNet_WeightActBias.onnx\"\n",
    "export_qonnx(model, torch.randn(1, 3, 32, 32), export_onnx_path,verbose=True)\n",
    "qonnx_cleanup(export_onnx_path, out_file=export_onnx_path)\n",
    "\n",
    "# torch.onnx.export(model, dummy_input, \"model.onnx\", verbose=True)\n",
    "\n",
    "model = ModelWrapper(export_onnx_path)\n",
    "model = model.transform(ConvertQONNXtoFINN())\n",
    "model = model.transform(InferShapes())\n",
    "model = model.transform(FoldConstants())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(GiveReadableTensorNames())\n",
    "model = model.transform(RemoveStaticGraphInputs())\n",
    "model.save(\"LeNet_WeightActBias_tidy.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "265542ba-0924-4277-b9c3-3c7fdfc24a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'LeNet_WeightActBias_tidy.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7412d5d47850>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finn.util.visualization import showInNetron\n",
    "showInNetron(\"LeNet_WeightActBias_tidy.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1211f0f2-cf63-424c-8284-12e8ac3f99ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inets/arish/FINN_arish/finn/deps/qonnx/src/qonnx/transformation/infer_data_layouts.py:127: UserWarning: Assuming 4D input is NCHW\n",
      "  warnings.warn(\"Assuming 4D input is NCHW\")\n"
     ]
    }
   ],
   "source": [
    "from finn.util.pytorch import ToTensor\n",
    "from qonnx.transformation.merge_onnx_models import MergeONNXModels\n",
    "from qonnx.core.datatype import DataType\n",
    "\n",
    "model = ModelWrapper(\"LeNet_WeightActBias_tidy.onnx\")\n",
    "global_inp_name = model.graph.input[0].name\n",
    "ishape = model.get_tensor_shape(global_inp_name)\n",
    "# preprocessing: torchvision's ToTensor divides uint8 inputs by 255\n",
    "totensor_pyt = ToTensor()\n",
    "chkpt_preproc_name = \"LeNet_WeightActBias_tidy_preproc.onnx\"\n",
    "export_qonnx(totensor_pyt, torch.randn(ishape), chkpt_preproc_name)\n",
    "qonnx_cleanup(chkpt_preproc_name, out_file=chkpt_preproc_name)\n",
    "pre_model = ModelWrapper(chkpt_preproc_name)\n",
    "pre_model = pre_model.transform(ConvertQONNXtoFINN())\n",
    "\n",
    "# join preprocessing and core model\n",
    "model = model.transform(MergeONNXModels(pre_model))\n",
    "# add input quantization annotation: UINT8 for all BNN-PYNQ models\n",
    "global_inp_name = model.graph.input[0].name\n",
    "model.set_tensor_datatype(global_inp_name, DataType[\"UINT8\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f3fad50-462e-4c9d-bdc6-521c67ae204c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qonnx.transformation.insert_topk import InsertTopK\n",
    "from qonnx.transformation.infer_datatypes import InferDataTypes\n",
    "\n",
    "# postprocessing: insert Top-1 node at the end\n",
    "model = model.transform(InsertTopK(k=1))\n",
    "chkpt_name = \"LeNet_WeightActBias_tidy_prepost.onnx\"\n",
    "# tidy-up again\n",
    "model = model.transform(InferShapes())\n",
    "model = model.transform(FoldConstants())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(GiveReadableTensorNames())\n",
    "model = model.transform(InferDataTypes())\n",
    "model = model.transform(RemoveStaticGraphInputs())\n",
    "model.save(chkpt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67d5c167-bdf5-40f0-9bee-47ff95af3123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving 'LeNet_WeightActBias_tidy_prepost.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7412d87950c0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finn.util.visualization import showInNetron\n",
    "showInNetron(\"LeNet_WeightActBias_tidy_prepost.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ba44ae-b9d0-492b-aa95-f2b7eec2a48e",
   "metadata": {},
   "source": [
    "### Streamline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5deb278f-4cbf-4f5e-b0b6-ada72ba5acfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.streamline import Streamline\n",
    "from qonnx.transformation.lower_convs_to_matmul import LowerConvsToMatMul\n",
    "from qonnx.transformation.bipolar_to_xnor import ConvertBipolarMatMulToXnorPopcount\n",
    "import finn.transformation.streamline.absorb as absorb\n",
    "from finn.transformation.streamline.reorder import MakeMaxPoolNHWC, MoveScalarLinearPastInvariants\n",
    "from qonnx.transformation.infer_data_layouts import InferDataLayouts\n",
    "from qonnx.transformation.general import RemoveUnusedTensors\n",
    "import finn.transformation.fpgadataflow.convert_to_hw_layers as to_hw\n",
    "from qonnx.transformation.general import GiveReadableTensorNames, GiveUniqueNodeNames, RemoveStaticGraphInputs\n",
    "from finn.transformation.streamline.round_thresholds import RoundAndClipThresholds\n",
    "\n",
    "\n",
    "model = ModelWrapper(\"LeNet_WeightActBias_tidy_prepost.onnx\")\n",
    "model = model.transform(MoveScalarLinearPastInvariants())\n",
    "model = model.transform(Streamline())\n",
    "model = model.transform(LowerConvsToMatMul())\n",
    "model = model.transform(MakeMaxPoolNHWC())\n",
    "model = model.transform(absorb.AbsorbTransposeIntoMultiThreshold())\n",
    "model = model.transform(ConvertBipolarMatMulToXnorPopcount())\n",
    "model = model.transform(Streamline())\n",
    "# model = model.transform(LowerConvsToMatMul())#\n",
    "# model = model.transform(MakeMaxPoolNHWC())#\n",
    "# model = model.transform(absorb.AbsorbTransposeIntoMultiThreshold())#\n",
    "# model = model.transform(ConvertBipolarMatMulToXnorPopcount())#\n",
    "# model = model.transform(Streamline())\n",
    "\n",
    "# model = model.transform(absorb.AbsorbAddIntoMultiThreshold())\n",
    "# model = model.transform(absorb.AbsorbMulIntoMultiThreshold())\n",
    "# absorb final add-mul nodes into TopK\n",
    "model = model.transform(absorb.AbsorbScalarMulAddIntoTopK())\n",
    "# model = model.transform(RoundAndClipThresholds())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "# model = model.transform(GiveReadableTensorNames())\n",
    "# model = model.transform(InferDataTypes())\n",
    "model = model.transform(Streamline())\n",
    "\n",
    "# bit of tidy-up\n",
    "model = model.transform(InferDataLayouts())\n",
    "model = model.transform(RemoveUnusedTensors())\n",
    "model.save(\"LeNet_WeightActBias_tidy_streamlined.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ffe376c8-cc70-4e56-b23e-1984288defae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving 'LeNet_WeightActBias_tidy_streamlined.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7412d5be4f10>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finn.util.visualization import showInNetron\n",
    "showInNetron(\"LeNet_WeightActBias_tidy_streamlined.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f301ee-e6fc-40e3-8cdd-725839a7bc9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6d87c74-07e6-4306-bb1a-2e986eee2276",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "cycle-free graph violated: partition depends on itself",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# infer tensor data layouts\u001b[39;00m\n\u001b[1;32m     30\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtransform(InferDataLayouts())\n\u001b[0;32m---> 32\u001b[0m parent_model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCreateDataflowPartition\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m parent_model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLeNet_WeightActBias_tidy_dataflow_parent.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m sdp_node \u001b[38;5;241m=\u001b[39m parent_model\u001b[38;5;241m.\u001b[39mget_nodes_by_op_type(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStreamingDataflowPartition\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/home/inets/arish/FINN_arish/finn/deps/qonnx/src/qonnx/core/modelwrapper.py:140\u001b[0m, in \u001b[0;36mModelWrapper.transform\u001b[0;34m(self, transformation, make_deepcopy, cleanup)\u001b[0m\n\u001b[1;32m    138\u001b[0m model_was_changed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m model_was_changed:\n\u001b[0;32m--> 140\u001b[0m     (transformed_model, model_was_changed) \u001b[38;5;241m=\u001b[39m \u001b[43mtransformation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformed_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cleanup:\n\u001b[1;32m    142\u001b[0m     transformed_model\u001b[38;5;241m.\u001b[39mcleanup()\n",
      "File \u001b[0;32m/home/inets/arish/FINN_arish/finn/src/finn/transformation/fpgadataflow/create_dataflow_partition.py:80\u001b[0m, in \u001b[0;36mCreateDataflowPartition.apply\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# first, use the generic partitioning functionality to split up the graph\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m parent_model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mPartitionFromLambda\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43massign_partition_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartition_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartition_model_dir\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# change node types to StreamingDataflowPartition\u001b[39;00m\n\u001b[1;32m     86\u001b[0m p_nodes \u001b[38;5;241m=\u001b[39m parent_model\u001b[38;5;241m.\u001b[39mget_nodes_by_op_type(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenericPartition\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/home/inets/arish/FINN_arish/finn/deps/qonnx/src/qonnx/core/modelwrapper.py:140\u001b[0m, in \u001b[0;36mModelWrapper.transform\u001b[0;34m(self, transformation, make_deepcopy, cleanup)\u001b[0m\n\u001b[1;32m    138\u001b[0m model_was_changed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m model_was_changed:\n\u001b[0;32m--> 140\u001b[0m     (transformed_model, model_was_changed) \u001b[38;5;241m=\u001b[39m \u001b[43mtransformation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformed_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cleanup:\n\u001b[1;32m    142\u001b[0m     transformed_model\u001b[38;5;241m.\u001b[39mcleanup()\n",
      "File \u001b[0;32m/home/inets/arish/FINN_arish/finn/deps/qonnx/src/qonnx/transformation/create_generic_partitions.py:119\u001b[0m, in \u001b[0;36mPartitionFromLambda.apply\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m to_check:\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m node \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m--> 119\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitioning(node) \u001b[38;5;241m!=\u001b[39m partition_id\n\u001b[1;32m    120\u001b[0m         ), \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mcycle-free graph violated: partition depends on itself\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    121\u001b[0m         \u001b[38;5;66;03m# print(node)\u001b[39;00m\n\u001b[1;32m    122\u001b[0m         predecessors \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfind_direct_predecessors(node)\n",
      "\u001b[0;31mAssertionError\u001b[0m: cycle-free graph violated: partition depends on itself"
     ]
    }
   ],
   "source": [
    "from finn.util.basic import pynq_part_map\n",
    "# change this if you have a different PYNQ board, see list above\n",
    "pynq_board = \"Pynq-Z1\"\n",
    "fpga_part = pynq_part_map[pynq_board]\n",
    "target_clk_ns = 10\n",
    "\n",
    "import finn.transformation.fpgadataflow.convert_to_hw_layers as to_hw\n",
    "from finn.transformation.fpgadataflow.create_dataflow_partition import (\n",
    "    CreateDataflowPartition,\n",
    ")\n",
    "from finn.transformation.move_reshape import RemoveCNVtoFCFlatten\n",
    "from finn.transformation.fpgadataflow.specialize_layers import SpecializeLayers\n",
    "from qonnx.custom_op.registry import getCustomOp\n",
    "from qonnx.transformation.infer_data_layouts import InferDataLayouts\n",
    "\n",
    "model = ModelWrapper(\"LeNet_WeightActBias_tidy_streamlined.onnx\")\n",
    "model = model.transform(to_hw.InferBinaryMatrixVectorActivation())\n",
    "model = model.transform(to_hw.InferQuantizedMatrixVectorActivation())\n",
    "# TopK to LabelSelect\n",
    "model = model.transform(to_hw.InferLabelSelectLayer())\n",
    "# input quantization (if any) to standalone thresholding\n",
    "model = model.transform(to_hw.InferThresholdingLayer())\n",
    "model = model.transform(to_hw.InferConvInpGen())\n",
    "model = model.transform(to_hw.InferStreamingMaxPool())\n",
    "# get rid of Reshape(-1, 1) operation between hw nodes\n",
    "model = model.transform(RemoveCNVtoFCFlatten())\n",
    "# get rid of Tranpose -> Tranpose identity seq\n",
    "model = model.transform(absorb.AbsorbConsecutiveTransposes())\n",
    "# infer tensor data layouts\n",
    "model = model.transform(InferDataLayouts())\n",
    "\n",
    "parent_model = model.transform(CreateDataflowPartition())\n",
    "parent_model.save(\"LeNet_WeightActBias_tidy_dataflow_parent.onnx\")\n",
    "sdp_node = parent_model.get_nodes_by_op_type(\"StreamingDataflowPartition\")[0]\n",
    "sdp_node = getCustomOp(sdp_node)\n",
    "dataflow_model_filename = sdp_node.get_nodeattr(\"model\")\n",
    "# save the dataflow partition with a different name for easier access\n",
    "# and specialize the layers to HLS variants\n",
    "dataflow_model = ModelWrapper(dataflow_model_filename)\n",
    "dataflow_model = dataflow_model.transform(SpecializeLayers(fpga_part))\n",
    "dataflow_model.save(\"LeNet_WeightActBias_dataflow_model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc793f4e-4e33-4a9e-8bf1-2b8b2f49e005",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
